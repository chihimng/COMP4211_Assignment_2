{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN from Pretrained encoder\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from tensorboardX import SummaryWriter\n",
    "from pa2_sample_code import get_datasets\n",
    "\n",
    "train_data, eval_data = get_datasets()\n",
    "\n",
    "# split train set into holdout_train and holdout_eval sets\n",
    "holdout_train_len = int(len(train_data) * 0.8)\n",
    "holdout_eval_len = len(train_data) - holdout_train_len\n",
    "holdout_train_data, holdout_eval_data = torch.utils.data.random_split(train_data, [holdout_train_len, holdout_eval_len])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CnnFromPretrained(nn.Module):\n",
    "    def __init__(self, n_hidden):\n",
    "        super(CnnFromPretrained, self).__init__()\n",
    "        \n",
    "        self.encoder = torch.load('pretrained_encoder.pt')['model']\n",
    "        \n",
    "        self.predictor = nn.Sequential(\n",
    "            nn.Linear(in_features=32, out_features=n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=n_hidden, out_features=47)\n",
    "        )\n",
    "        \n",
    "        self.loss_func = nn.CrossEntropyLoss(reduction='sum')\n",
    "        \n",
    "    def forward(self, in_data):\n",
    "        img_features = self.encoder(in_data).view(in_data.size(0), 32)\n",
    "        logits = self.predictor(img_features)\n",
    "        return logits\n",
    "\n",
    "    def loss(self, logits, labels):\n",
    "        return self.loss_func(logits, labels) / logits.size(0)\n",
    "    \n",
    "    def top_k_acc(self, logits, labels, k=1):\n",
    "        _, k_labels_pred = torch.topk(logits, k=k, dim=1) # shape (n, k)\n",
    "        k_labels = labels.unsqueeze(dim=1).expand(-1, k) # broadcast from (n) to (n, 1) to (n, k)\n",
    "        # flatten tensors for comparison\n",
    "        k_labels_pred_flat = k_labels_pred.reshape(1,-1).squeeze()\n",
    "        k_labels_flat = k_labels.reshape(1,-1).squeeze()\n",
    "        # get num_correct in float\n",
    "        num_correct = k_labels_pred_flat.eq(k_labels_flat).sum(0).float().item()\n",
    "        return num_correct / labels.size(0)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refactored runner function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(model, loaders, optimizer, writer, num_epoch=10, device='cpu'):\n",
    "    def run_epoch(mode):\n",
    "        epoch_loss = 0.0\n",
    "        epoch_top1 = 0.0\n",
    "        epoch_top3 = 0.0\n",
    "        for i, batch in enumerate(loaders[mode], 0):\n",
    "            in_data, labels = batch\n",
    "            in_data, labels = in_data.to(device), labels.to(device)\n",
    "\n",
    "            if mode == 'train':\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            logits = model(in_data)\n",
    "            batch_loss = model.loss(logits, labels)\n",
    "            batch_top1 = model.top_k_acc(logits, labels, k=1)\n",
    "            batch_top3 = model.top_k_acc(logits, labels, k=3)\n",
    "\n",
    "            epoch_loss += batch_loss.item()\n",
    "            epoch_top1 += batch_top1\n",
    "            epoch_top3 += batch_top3\n",
    "\n",
    "            if mode == 'train':\n",
    "                batch_loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        # sum of all batchs / num of batches\n",
    "        epoch_loss /= i + 1 \n",
    "        epoch_top1 /= i + 1\n",
    "        epoch_top3 /= i + 1\n",
    "        \n",
    "        print('epoch %d %s loss %.4f top1 %.4f top3 %.4f' % (epoch, mode, epoch_loss, epoch_top1, epoch_top3))\n",
    "        # log to tensorboard\n",
    "        if not (writer is None):\n",
    "            writer.add_scalars('%s_loss' % model.__class__.__name__,\n",
    "                         tag_scalar_dict={mode: epoch_loss}, \n",
    "                         global_step=epoch)\n",
    "            writer.add_scalars('%s_top1' % model.__class__.__name__,\n",
    "                         tag_scalar_dict={mode: epoch_top1}, \n",
    "                         global_step=epoch)\n",
    "            writer.add_scalars('%s_top3' % model.__class__.__name__,\n",
    "                         tag_scalar_dict={mode: epoch_top3}, \n",
    "                         global_step=epoch)\n",
    "    for epoch in range(num_epoch):\n",
    "        run_epoch('train')\n",
    "        run_epoch('eval')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Holdout validation for choosing hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32_adam_0.001\n",
      "epoch 0 train loss 3.0870 top1 0.2599 top3 0.4856\n",
      "epoch 0 eval loss 2.3301 top1 0.4100 top3 0.6902\n",
      "epoch 1 train loss 1.9963 top1 0.4715 top3 0.7420\n",
      "epoch 1 eval loss 1.7845 top1 0.5110 top3 0.7787\n",
      "epoch 2 train loss 1.6394 top1 0.5436 top3 0.7996\n",
      "epoch 2 eval loss 1.5478 top1 0.5585 top3 0.8188\n",
      "epoch 3 train loss 1.4738 top1 0.5769 top3 0.8260\n",
      "epoch 3 eval loss 1.4218 top1 0.5889 top3 0.8335\n",
      "epoch 4 train loss 1.3811 top1 0.5967 top3 0.8401\n",
      "epoch 4 eval loss 1.3566 top1 0.6036 top3 0.8475\n",
      "epoch 5 train loss 1.3201 top1 0.6115 top3 0.8478\n",
      "epoch 5 eval loss 1.3087 top1 0.6130 top3 0.8548\n",
      "epoch 6 train loss 1.2764 top1 0.6230 top3 0.8543\n",
      "epoch 6 eval loss 1.2644 top1 0.6258 top3 0.8608\n",
      "epoch 7 train loss 1.2434 top1 0.6300 top3 0.8605\n",
      "epoch 7 eval loss 1.2343 top1 0.6368 top3 0.8642\n",
      "epoch 8 train loss 1.2188 top1 0.6368 top3 0.8637\n",
      "epoch 8 eval loss 1.2125 top1 0.6384 top3 0.8685\n",
      "epoch 9 train loss 1.1974 top1 0.6428 top3 0.8675\n",
      "epoch 9 eval loss 1.1922 top1 0.6445 top3 0.8712\n",
      "32_sgd_0.1\n",
      "epoch 0 train loss 3.2599 top1 0.2066 top3 0.4126\n",
      "epoch 0 eval loss 2.3023 top1 0.4012 top3 0.6829\n",
      "epoch 1 train loss 1.8164 top1 0.4983 top3 0.7680\n",
      "epoch 1 eval loss 1.5317 top1 0.5609 top3 0.8144\n",
      "epoch 2 train loss 1.4164 top1 0.5801 top3 0.8319\n",
      "epoch 2 eval loss 1.3313 top1 0.6008 top3 0.8482\n",
      "epoch 3 train loss 1.2929 top1 0.6124 top3 0.8527\n",
      "epoch 3 eval loss 1.2483 top1 0.6220 top3 0.8611\n",
      "epoch 4 train loss 1.2343 top1 0.6274 top3 0.8621\n",
      "epoch 4 eval loss 1.1984 top1 0.6411 top3 0.8675\n",
      "epoch 5 train loss 1.2011 top1 0.6357 top3 0.8670\n",
      "epoch 5 eval loss 1.1940 top1 0.6347 top3 0.8716\n",
      "epoch 6 train loss 1.1759 top1 0.6423 top3 0.8721\n",
      "epoch 6 eval loss 1.1649 top1 0.6504 top3 0.8726\n",
      "epoch 7 train loss 1.1558 top1 0.6508 top3 0.8751\n",
      "epoch 7 eval loss 1.1422 top1 0.6576 top3 0.8770\n",
      "epoch 8 train loss 1.1407 top1 0.6534 top3 0.8774\n",
      "epoch 8 eval loss 1.1353 top1 0.6597 top3 0.8797\n",
      "epoch 9 train loss 1.1269 top1 0.6583 top3 0.8811\n",
      "epoch 9 eval loss 1.1189 top1 0.6620 top3 0.8826\n",
      "32_sgd_0.01\n",
      "epoch 0 train loss 3.8433 top1 0.0216 top3 0.0700\n",
      "epoch 0 eval loss 3.8252 top1 0.0236 top3 0.0870\n",
      "epoch 1 train loss 3.8017 top1 0.0585 top3 0.1619\n",
      "epoch 1 eval loss 3.7768 top1 0.0820 top3 0.2317\n",
      "epoch 2 train loss 3.7375 top1 0.1461 top3 0.3264\n",
      "epoch 2 eval loss 3.6940 top1 0.1935 top3 0.4028\n",
      "epoch 3 train loss 3.6259 top1 0.2167 top3 0.4572\n",
      "epoch 3 eval loss 3.5528 top1 0.2336 top3 0.4871\n",
      "epoch 4 train loss 3.4452 top1 0.2531 top3 0.5321\n",
      "epoch 4 eval loss 3.3396 top1 0.2649 top3 0.5395\n",
      "epoch 5 train loss 3.2030 top1 0.3027 top3 0.5838\n",
      "epoch 5 eval loss 3.0809 top1 0.3124 top3 0.6038\n",
      "epoch 6 train loss 2.9381 top1 0.3543 top3 0.6322\n",
      "epoch 6 eval loss 2.8192 top1 0.3817 top3 0.6442\n",
      "epoch 7 train loss 2.6841 top1 0.4047 top3 0.6695\n",
      "epoch 7 eval loss 2.5782 top1 0.4177 top3 0.6910\n",
      "epoch 8 train loss 2.4557 top1 0.4394 top3 0.7051\n",
      "epoch 8 eval loss 2.3649 top1 0.4466 top3 0.7138\n",
      "epoch 9 train loss 2.2581 top1 0.4674 top3 0.7313\n",
      "epoch 9 eval loss 2.1829 top1 0.4837 top3 0.7401\n",
      "64_adam_0.001\n",
      "epoch 0 train loss 2.6990 top1 0.3645 top3 0.6133\n",
      "epoch 0 eval loss 1.8594 top1 0.5153 top3 0.7779\n",
      "epoch 1 train loss 1.6066 top1 0.5579 top3 0.8118\n",
      "epoch 1 eval loss 1.4644 top1 0.5847 top3 0.8306\n",
      "epoch 2 train loss 1.3836 top1 0.5984 top3 0.8413\n",
      "epoch 2 eval loss 1.3301 top1 0.6169 top3 0.8485\n",
      "epoch 3 train loss 1.2863 top1 0.6216 top3 0.8551\n",
      "epoch 3 eval loss 1.2619 top1 0.6254 top3 0.8600\n",
      "epoch 4 train loss 1.2293 top1 0.6339 top3 0.8631\n",
      "epoch 4 eval loss 1.2095 top1 0.6393 top3 0.8661\n",
      "epoch 5 train loss 1.1911 top1 0.6454 top3 0.8691\n",
      "epoch 5 eval loss 1.1728 top1 0.6514 top3 0.8697\n",
      "epoch 6 train loss 1.1651 top1 0.6507 top3 0.8735\n",
      "epoch 6 eval loss 1.1633 top1 0.6506 top3 0.8737\n",
      "epoch 7 train loss 1.1466 top1 0.6572 top3 0.8758\n",
      "epoch 7 eval loss 1.1486 top1 0.6586 top3 0.8726\n",
      "epoch 8 train loss 1.1320 top1 0.6599 top3 0.8786\n",
      "epoch 8 eval loss 1.1339 top1 0.6628 top3 0.8755\n",
      "epoch 9 train loss 1.1202 top1 0.6638 top3 0.8806\n",
      "epoch 9 eval loss 1.1212 top1 0.6656 top3 0.8801\n",
      "64_sgd_0.1\n",
      "epoch 0 train loss 3.1241 top1 0.2545 top3 0.4727\n",
      "epoch 0 eval loss 2.0942 top1 0.4707 top3 0.7380\n",
      "epoch 1 train loss 1.6526 top1 0.5435 top3 0.8010\n",
      "epoch 1 eval loss 1.4212 top1 0.5986 top3 0.8285\n",
      "epoch 2 train loss 1.3275 top1 0.6092 top3 0.8487\n",
      "epoch 2 eval loss 1.2734 top1 0.6172 top3 0.8551\n",
      "epoch 3 train loss 1.2232 top1 0.6324 top3 0.8640\n",
      "epoch 3 eval loss 1.1826 top1 0.6450 top3 0.8694\n",
      "epoch 4 train loss 1.1714 top1 0.6467 top3 0.8722\n",
      "epoch 4 eval loss 1.1628 top1 0.6465 top3 0.8742\n",
      "epoch 5 train loss 1.1401 top1 0.6566 top3 0.8763\n",
      "epoch 5 eval loss 1.1255 top1 0.6581 top3 0.8793\n",
      "epoch 6 train loss 1.1149 top1 0.6596 top3 0.8820\n",
      "epoch 6 eval loss 1.1093 top1 0.6631 top3 0.8810\n",
      "epoch 7 train loss 1.0955 top1 0.6669 top3 0.8847\n",
      "epoch 7 eval loss 1.1014 top1 0.6633 top3 0.8818\n",
      "epoch 8 train loss 1.0772 top1 0.6730 top3 0.8873\n",
      "epoch 8 eval loss 1.0748 top1 0.6745 top3 0.8864\n",
      "epoch 9 train loss 1.0642 top1 0.6765 top3 0.8895\n",
      "epoch 9 eval loss 1.0626 top1 0.6746 top3 0.8911\n",
      "64_sgd_0.01\n",
      "epoch 0 train loss 3.8322 top1 0.0327 top3 0.1051\n",
      "epoch 0 eval loss 3.8070 top1 0.0610 top3 0.1642\n",
      "epoch 1 train loss 3.7733 top1 0.1099 top3 0.2694\n",
      "epoch 1 eval loss 3.7384 top1 0.1404 top3 0.3504\n",
      "epoch 2 train loss 3.6794 top1 0.2076 top3 0.4482\n",
      "epoch 2 eval loss 3.6182 top1 0.2203 top3 0.4891\n",
      "epoch 3 train loss 3.5200 top1 0.2741 top3 0.5521\n",
      "epoch 3 eval loss 3.4221 top1 0.2806 top3 0.5607\n",
      "epoch 4 train loss 3.2818 top1 0.3208 top3 0.6081\n",
      "epoch 4 eval loss 3.1515 top1 0.3489 top3 0.6349\n",
      "epoch 5 train loss 2.9927 top1 0.3772 top3 0.6568\n",
      "epoch 5 eval loss 2.8584 top1 0.3884 top3 0.6643\n",
      "epoch 6 train loss 2.7067 top1 0.4200 top3 0.6913\n",
      "epoch 6 eval loss 2.5887 top1 0.4311 top3 0.6954\n",
      "epoch 7 train loss 2.4538 top1 0.4594 top3 0.7225\n",
      "epoch 7 eval loss 2.3556 top1 0.4601 top3 0.7213\n",
      "epoch 8 train loss 2.2406 top1 0.4846 top3 0.7445\n",
      "epoch 8 eval loss 2.1622 top1 0.4934 top3 0.7512\n",
      "epoch 9 train loss 2.0660 top1 0.5074 top3 0.7627\n",
      "epoch 9 eval loss 2.0010 top1 0.5098 top3 0.7702\n"
     ]
    }
   ],
   "source": [
    "for n_hidden in [32, 64]:\n",
    "    for optim_conf in [\n",
    "        {'optim':'adam', 'lr':0.001},\n",
    "        {'optim':'sgd', 'lr':0.1},\n",
    "        {'optim':'sgd', 'lr':0.01}\n",
    "    ]:\n",
    "        model = CnnFromPretrained(n_hidden=n_hidden)\n",
    "        if optim_conf['optim'] == 'adam':\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=optim_conf['lr'])\n",
    "        else:\n",
    "            optimizer = torch.optim.SGD(model.parameters(), lr=optim_conf['lr'])\n",
    "        conf_str = str(n_hidden)+'_'+optim_conf['optim']+'_'+str(optim_conf['lr'])\n",
    "        print(conf_str)\n",
    "        run(\n",
    "            model=model,\n",
    "            loaders={\n",
    "                'train': torch.utils.data.DataLoader(holdout_train_data, batch_size=32, shuffle=True),\n",
    "                'eval': torch.utils.data.DataLoader(holdout_eval_data, batch_size=32, shuffle=True)\n",
    "            },\n",
    "            optimizer=optimizer, \n",
    "            writer=SummaryWriter('./logs/cnn_pretrained/%s' % (conf_str)), \n",
    "            num_epoch=10, \n",
    "            device='cpu'\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training final model\n",
    "Hyper-parameter selected:\n",
    "- Hidden layers: 64\n",
    "- Optimization: SGD, learning rate 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_0\n",
      "epoch 0 train loss 2.8651 top1 0.3119 top3 0.5411\n",
      "epoch 0 eval loss 1.8038 top1 0.5174 top3 0.7787\n",
      "epoch 1 train loss 1.4926 top1 0.5711 top3 0.8227\n",
      "epoch 1 eval loss 1.3382 top1 0.6087 top3 0.8438\n",
      "epoch 2 train loss 1.2616 top1 0.6235 top3 0.8580\n",
      "epoch 2 eval loss 1.2344 top1 0.6306 top3 0.8618\n",
      "epoch 3 train loss 1.1841 top1 0.6424 top3 0.8697\n",
      "epoch 3 eval loss 1.1815 top1 0.6436 top3 0.8739\n",
      "epoch 4 train loss 1.1422 top1 0.6517 top3 0.8777\n",
      "epoch 4 eval loss 1.1355 top1 0.6635 top3 0.8787\n",
      "epoch 5 train loss 1.1138 top1 0.6629 top3 0.8821\n",
      "epoch 5 eval loss 1.1265 top1 0.6592 top3 0.8803\n",
      "epoch 6 train loss 1.0933 top1 0.6688 top3 0.8842\n",
      "epoch 6 eval loss 1.1069 top1 0.6617 top3 0.8868\n",
      "epoch 7 train loss 1.0763 top1 0.6714 top3 0.8888\n",
      "epoch 7 eval loss 1.0773 top1 0.6740 top3 0.8864\n",
      "epoch 8 train loss 1.0607 top1 0.6762 top3 0.8920\n",
      "epoch 8 eval loss 1.0669 top1 0.6785 top3 0.8877\n",
      "epoch 9 train loss 1.0451 top1 0.6797 top3 0.8945\n",
      "epoch 9 eval loss 1.0448 top1 0.6852 top3 0.8924\n",
      "epoch 10 train loss 1.0311 top1 0.6828 top3 0.8953\n",
      "epoch 10 eval loss 1.0313 top1 0.6883 top3 0.8953\n",
      "epoch 11 train loss 1.0187 top1 0.6866 top3 0.8981\n",
      "epoch 11 eval loss 1.0197 top1 0.6901 top3 0.8982\n",
      "epoch 12 train loss 1.0053 top1 0.6906 top3 0.8993\n",
      "epoch 12 eval loss 1.0116 top1 0.6905 top3 0.8990\n",
      "epoch 13 train loss 0.9934 top1 0.6933 top3 0.9030\n",
      "epoch 13 eval loss 1.0124 top1 0.6927 top3 0.8973\n",
      "epoch 14 train loss 0.9817 top1 0.6974 top3 0.9045\n",
      "epoch 14 eval loss 0.9986 top1 0.6968 top3 0.9004\n",
      "epoch 15 train loss 0.9713 top1 0.7002 top3 0.9057\n",
      "epoch 15 eval loss 0.9955 top1 0.6982 top3 0.9009\n",
      "epoch 16 train loss 0.9607 top1 0.7015 top3 0.9072\n",
      "epoch 16 eval loss 0.9782 top1 0.6972 top3 0.9034\n",
      "epoch 17 train loss 0.9489 top1 0.7041 top3 0.9095\n",
      "epoch 17 eval loss 0.9919 top1 0.6927 top3 0.9028\n",
      "epoch 18 train loss 0.9405 top1 0.7084 top3 0.9109\n",
      "epoch 18 eval loss 0.9610 top1 0.7064 top3 0.9072\n",
      "epoch 19 train loss 0.9317 top1 0.7092 top3 0.9123\n",
      "epoch 19 eval loss 0.9650 top1 0.7037 top3 0.9066\n",
      "final_1\n",
      "epoch 0 train loss 2.8631 top1 0.3164 top3 0.5463\n",
      "epoch 0 eval loss 1.7853 top1 0.5251 top3 0.7806\n",
      "epoch 1 train loss 1.4942 top1 0.5691 top3 0.8227\n",
      "epoch 1 eval loss 1.3388 top1 0.6133 top3 0.8465\n",
      "epoch 2 train loss 1.2673 top1 0.6224 top3 0.8579\n",
      "epoch 2 eval loss 1.2440 top1 0.6243 top3 0.8569\n",
      "epoch 3 train loss 1.1856 top1 0.6433 top3 0.8701\n",
      "epoch 3 eval loss 1.1734 top1 0.6442 top3 0.8730\n",
      "epoch 4 train loss 1.1402 top1 0.6554 top3 0.8784\n",
      "epoch 4 eval loss 1.1440 top1 0.6561 top3 0.8767\n",
      "epoch 5 train loss 1.1115 top1 0.6627 top3 0.8814\n",
      "epoch 5 eval loss 1.1231 top1 0.6603 top3 0.8807\n",
      "epoch 6 train loss 1.0866 top1 0.6711 top3 0.8863\n",
      "epoch 6 eval loss 1.0895 top1 0.6746 top3 0.8845\n",
      "epoch 7 train loss 1.0665 top1 0.6764 top3 0.8894\n",
      "epoch 7 eval loss 1.0761 top1 0.6729 top3 0.8870\n",
      "epoch 8 train loss 1.0489 top1 0.6808 top3 0.8927\n",
      "epoch 8 eval loss 1.0576 top1 0.6785 top3 0.8927\n",
      "epoch 9 train loss 1.0325 top1 0.6856 top3 0.8957\n",
      "epoch 9 eval loss 1.0486 top1 0.6824 top3 0.8915\n",
      "epoch 10 train loss 1.0183 top1 0.6868 top3 0.8978\n",
      "epoch 10 eval loss 1.0406 top1 0.6773 top3 0.8965\n",
      "epoch 11 train loss 1.0031 top1 0.6913 top3 0.8982\n",
      "epoch 11 eval loss 1.0139 top1 0.6918 top3 0.8991\n",
      "epoch 12 train loss 0.9899 top1 0.6959 top3 0.9018\n",
      "epoch 12 eval loss 1.0142 top1 0.6890 top3 0.9010\n",
      "epoch 13 train loss 0.9774 top1 0.6984 top3 0.9040\n",
      "epoch 13 eval loss 1.0014 top1 0.6961 top3 0.9012\n",
      "epoch 14 train loss 0.9658 top1 0.7008 top3 0.9064\n",
      "epoch 14 eval loss 0.9842 top1 0.6976 top3 0.9048\n",
      "epoch 15 train loss 0.9566 top1 0.7041 top3 0.9072\n",
      "epoch 15 eval loss 0.9762 top1 0.6994 top3 0.9066\n",
      "epoch 16 train loss 0.9460 top1 0.7051 top3 0.9101\n",
      "epoch 16 eval loss 0.9672 top1 0.7021 top3 0.9082\n",
      "epoch 17 train loss 0.9383 top1 0.7082 top3 0.9112\n",
      "epoch 17 eval loss 0.9537 top1 0.7082 top3 0.9110\n",
      "epoch 18 train loss 0.9291 top1 0.7105 top3 0.9126\n",
      "epoch 18 eval loss 0.9579 top1 0.7035 top3 0.9094\n",
      "epoch 19 train loss 0.9220 top1 0.7122 top3 0.9130\n",
      "epoch 19 eval loss 0.9483 top1 0.7134 top3 0.9108\n",
      "final_2\n",
      "epoch 0 train loss 2.9213 top1 0.2877 top3 0.5186\n",
      "epoch 0 eval loss 1.8352 top1 0.5100 top3 0.7771\n",
      "epoch 1 train loss 1.5124 top1 0.5684 top3 0.8199\n",
      "epoch 1 eval loss 1.3696 top1 0.5940 top3 0.8406\n",
      "epoch 2 train loss 1.2609 top1 0.6259 top3 0.8577\n",
      "epoch 2 eval loss 1.2322 top1 0.6363 top3 0.8608\n",
      "epoch 3 train loss 1.1785 top1 0.6445 top3 0.8707\n",
      "epoch 3 eval loss 1.1675 top1 0.6525 top3 0.8723\n",
      "epoch 4 train loss 1.1352 top1 0.6568 top3 0.8787\n",
      "epoch 4 eval loss 1.1315 top1 0.6542 top3 0.8802\n",
      "epoch 5 train loss 1.1048 top1 0.6649 top3 0.8839\n",
      "epoch 5 eval loss 1.1098 top1 0.6617 top3 0.8824\n",
      "epoch 6 train loss 1.0814 top1 0.6681 top3 0.8874\n",
      "epoch 6 eval loss 1.0819 top1 0.6730 top3 0.8873\n",
      "epoch 7 train loss 1.0622 top1 0.6759 top3 0.8904\n",
      "epoch 7 eval loss 1.0698 top1 0.6806 top3 0.8883\n",
      "epoch 8 train loss 1.0440 top1 0.6820 top3 0.8938\n",
      "epoch 8 eval loss 1.0425 top1 0.6848 top3 0.8936\n",
      "epoch 9 train loss 1.0268 top1 0.6863 top3 0.8958\n",
      "epoch 9 eval loss 1.0329 top1 0.6855 top3 0.8930\n",
      "epoch 10 train loss 1.0121 top1 0.6880 top3 0.8998\n",
      "epoch 10 eval loss 1.0188 top1 0.6916 top3 0.8976\n",
      "epoch 11 train loss 0.9996 top1 0.6906 top3 0.9022\n",
      "epoch 11 eval loss 1.0158 top1 0.6917 top3 0.8971\n",
      "epoch 12 train loss 0.9848 top1 0.6951 top3 0.9043\n",
      "epoch 12 eval loss 1.0207 top1 0.6909 top3 0.9003\n",
      "epoch 13 train loss 0.9719 top1 0.6990 top3 0.9070\n",
      "epoch 13 eval loss 0.9905 top1 0.6939 top3 0.9042\n",
      "epoch 14 train loss 0.9598 top1 0.7023 top3 0.9071\n",
      "epoch 14 eval loss 0.9720 top1 0.7031 top3 0.9054\n",
      "epoch 15 train loss 0.9478 top1 0.7052 top3 0.9096\n",
      "epoch 15 eval loss 0.9869 top1 0.6945 top3 0.9034\n",
      "epoch 16 train loss 0.9360 top1 0.7082 top3 0.9122\n",
      "epoch 16 eval loss 0.9418 top1 0.7121 top3 0.9115\n",
      "epoch 17 train loss 0.9262 top1 0.7113 top3 0.9135\n",
      "epoch 17 eval loss 0.9448 top1 0.7082 top3 0.9095\n",
      "epoch 18 train loss 0.9167 top1 0.7138 top3 0.9147\n",
      "epoch 18 eval loss 0.9339 top1 0.7103 top3 0.9130\n",
      "epoch 19 train loss 0.9060 top1 0.7154 top3 0.9167\n",
      "epoch 19 eval loss 0.9204 top1 0.7147 top3 0.9144\n",
      "final_3\n",
      "epoch 0 train loss 2.8212 top1 0.3235 top3 0.5551\n",
      "epoch 0 eval loss 1.7442 top1 0.5395 top3 0.7894\n",
      "epoch 1 train loss 1.4622 top1 0.5803 top3 0.8294\n",
      "epoch 1 eval loss 1.3184 top1 0.6191 top3 0.8454\n",
      "epoch 2 train loss 1.2469 top1 0.6289 top3 0.8614\n",
      "epoch 2 eval loss 1.2162 top1 0.6343 top3 0.8664\n",
      "epoch 3 train loss 1.1742 top1 0.6460 top3 0.8717\n",
      "epoch 3 eval loss 1.1637 top1 0.6481 top3 0.8734\n",
      "epoch 4 train loss 1.1363 top1 0.6571 top3 0.8790\n",
      "epoch 4 eval loss 1.1260 top1 0.6647 top3 0.8792\n",
      "epoch 5 train loss 1.1071 top1 0.6642 top3 0.8825\n",
      "epoch 5 eval loss 1.1331 top1 0.6547 top3 0.8793\n",
      "epoch 6 train loss 1.0863 top1 0.6684 top3 0.8864\n",
      "epoch 6 eval loss 1.0962 top1 0.6697 top3 0.8847\n",
      "epoch 7 train loss 1.0666 top1 0.6753 top3 0.8900\n",
      "epoch 7 eval loss 1.0666 top1 0.6768 top3 0.8921\n",
      "epoch 8 train loss 1.0493 top1 0.6787 top3 0.8927\n",
      "epoch 8 eval loss 1.1031 top1 0.6663 top3 0.8851\n",
      "epoch 9 train loss 1.0357 top1 0.6823 top3 0.8961\n",
      "epoch 9 eval loss 1.0432 top1 0.6863 top3 0.8935\n",
      "epoch 10 train loss 1.0226 top1 0.6876 top3 0.8976\n",
      "epoch 10 eval loss 1.0548 top1 0.6782 top3 0.8928\n",
      "epoch 11 train loss 1.0079 top1 0.6916 top3 0.8987\n",
      "epoch 11 eval loss 1.0235 top1 0.6909 top3 0.8970\n",
      "epoch 12 train loss 0.9944 top1 0.6939 top3 0.9024\n",
      "epoch 12 eval loss 1.0046 top1 0.6974 top3 0.9003\n",
      "epoch 13 train loss 0.9817 top1 0.6982 top3 0.9044\n",
      "epoch 13 eval loss 0.9941 top1 0.6985 top3 0.9021\n",
      "epoch 14 train loss 0.9693 top1 0.6997 top3 0.9066\n",
      "epoch 14 eval loss 0.9814 top1 0.7025 top3 0.9038\n",
      "epoch 15 train loss 0.9572 top1 0.7046 top3 0.9077\n",
      "epoch 15 eval loss 0.9757 top1 0.6993 top3 0.9066\n",
      "epoch 16 train loss 0.9466 top1 0.7069 top3 0.9105\n",
      "epoch 16 eval loss 0.9761 top1 0.7021 top3 0.9045\n",
      "epoch 17 train loss 0.9354 top1 0.7088 top3 0.9109\n",
      "epoch 17 eval loss 0.9517 top1 0.7097 top3 0.9097\n",
      "epoch 18 train loss 0.9261 top1 0.7127 top3 0.9132\n",
      "epoch 18 eval loss 0.9362 top1 0.7131 top3 0.9114\n",
      "epoch 19 train loss 0.9150 top1 0.7150 top3 0.9158\n",
      "epoch 19 eval loss 0.9528 top1 0.7064 top3 0.9100\n",
      "final_4\n",
      "epoch 0 train loss 2.8323 top1 0.3205 top3 0.5515\n",
      "epoch 0 eval loss 1.7701 top1 0.5181 top3 0.7859\n",
      "epoch 1 train loss 1.4784 top1 0.5773 top3 0.8251\n",
      "epoch 1 eval loss 1.3340 top1 0.6113 top3 0.8450\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 train loss 1.2560 top1 0.6254 top3 0.8587\n",
      "epoch 2 eval loss 1.2397 top1 0.6302 top3 0.8591\n",
      "epoch 3 train loss 1.1766 top1 0.6462 top3 0.8712\n",
      "epoch 3 eval loss 1.1538 top1 0.6559 top3 0.8722\n",
      "epoch 4 train loss 1.1322 top1 0.6567 top3 0.8784\n",
      "epoch 4 eval loss 1.1412 top1 0.6582 top3 0.8766\n",
      "epoch 5 train loss 1.1027 top1 0.6664 top3 0.8838\n",
      "epoch 5 eval loss 1.1308 top1 0.6561 top3 0.8799\n",
      "epoch 6 train loss 1.0793 top1 0.6718 top3 0.8872\n",
      "epoch 6 eval loss 1.1070 top1 0.6609 top3 0.8860\n",
      "epoch 7 train loss 1.0589 top1 0.6769 top3 0.8911\n",
      "epoch 7 eval loss 1.0695 top1 0.6718 top3 0.8897\n",
      "epoch 8 train loss 1.0388 top1 0.6815 top3 0.8944\n",
      "epoch 8 eval loss 1.0615 top1 0.6774 top3 0.8931\n",
      "epoch 9 train loss 1.0233 top1 0.6855 top3 0.8973\n",
      "epoch 9 eval loss 1.0376 top1 0.6854 top3 0.8953\n",
      "epoch 10 train loss 1.0060 top1 0.6896 top3 0.9001\n",
      "epoch 10 eval loss 1.0197 top1 0.6857 top3 0.8985\n",
      "epoch 11 train loss 0.9926 top1 0.6922 top3 0.9030\n",
      "epoch 11 eval loss 1.0287 top1 0.6867 top3 0.8952\n",
      "epoch 12 train loss 0.9775 top1 0.6982 top3 0.9057\n",
      "epoch 12 eval loss 0.9862 top1 0.6984 top3 0.9028\n",
      "epoch 13 train loss 0.9640 top1 0.7026 top3 0.9072\n",
      "epoch 13 eval loss 0.9806 top1 0.6988 top3 0.9070\n",
      "epoch 14 train loss 0.9531 top1 0.7043 top3 0.9093\n",
      "epoch 14 eval loss 0.9838 top1 0.6951 top3 0.9060\n",
      "epoch 15 train loss 0.9402 top1 0.7078 top3 0.9113\n",
      "epoch 15 eval loss 0.9564 top1 0.7036 top3 0.9086\n",
      "epoch 16 train loss 0.9306 top1 0.7099 top3 0.9123\n",
      "epoch 16 eval loss 0.9551 top1 0.7056 top3 0.9116\n",
      "epoch 17 train loss 0.9189 top1 0.7133 top3 0.9149\n",
      "epoch 17 eval loss 0.9381 top1 0.7153 top3 0.9110\n",
      "epoch 18 train loss 0.9093 top1 0.7145 top3 0.9162\n",
      "epoch 18 eval loss 0.9492 top1 0.7135 top3 0.9100\n",
      "epoch 19 train loss 0.9020 top1 0.7168 top3 0.9163\n",
      "epoch 19 eval loss 0.9254 top1 0.7147 top3 0.9140\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    model = CnnFromPretrained(n_hidden=64)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "    conf_str = 'final_'+str(i)\n",
    "    print(conf_str)\n",
    "    run(\n",
    "        model=model,\n",
    "        loaders={\n",
    "            'train': torch.utils.data.DataLoader(train_data, batch_size=32, shuffle=True),\n",
    "            'eval': torch.utils.data.DataLoader(eval_data, batch_size=32, shuffle=True)\n",
    "        },\n",
    "        optimizer=optimizer, \n",
    "        writer=SummaryWriter('./logs/cnn_pretrained/%s' % (conf_str)), \n",
    "        num_epoch=20, \n",
    "        device='cpu'\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
