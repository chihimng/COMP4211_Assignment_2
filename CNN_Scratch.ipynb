{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN from scratch\n",
    "\n",
    "## Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from tensorboardX import SummaryWriter\n",
    "from pa2_sample_code import get_datasets\n",
    "\n",
    "train_data, eval_data = get_datasets()\n",
    "\n",
    "# split train set into holdout_train and holdout_eval sets\n",
    "holdout_train_len = int(len(train_data) * 0.8)\n",
    "holdout_eval_len = len(train_data) - holdout_train_len\n",
    "holdout_train_data, holdout_eval_data = torch.utils.data.random_split(train_data, [holdout_train_len, holdout_eval_len])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declare model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CnnClassifier(nn.Module):\n",
    "    def __init__(self, n_hidden):\n",
    "        super(CnnClassifier, self).__init__()\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=4, kernel_size=3, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=4, out_channels=8, kernel_size=3, stride=2, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3, stride=2, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=1, padding=0),\n",
    "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=0),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.predictor = nn.Sequential(\n",
    "            nn.Linear(in_features=32, out_features=n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=n_hidden, out_features=47)\n",
    "        )\n",
    "        \n",
    "        self.loss_func = nn.CrossEntropyLoss(reduction='sum')\n",
    "        \n",
    "    def forward(self, in_data):\n",
    "        img_features = self.encoder(in_data).view(in_data.size(0), 32)\n",
    "        logits = self.predictor(img_features)\n",
    "        return logits\n",
    "\n",
    "    def loss(self, logits, labels):\n",
    "        return self.loss_func(logits, labels) / logits.size(0)\n",
    "    \n",
    "    def top_k_acc(self, logits, labels, k=1):\n",
    "        _, k_labels_pred = torch.topk(logits, k=k, dim=1) # shape (n, k)\n",
    "        k_labels = labels.unsqueeze(dim=1).expand(-1, k) # broadcast from (n) to (n, 1) to (n, k)\n",
    "        # flatten tensors for comparison\n",
    "        k_labels_pred_flat = k_labels_pred.reshape(1,-1).squeeze()\n",
    "        k_labels_flat = k_labels.reshape(1,-1).squeeze()\n",
    "        # get num_correct in float\n",
    "        num_correct = k_labels_pred_flat.eq(k_labels_flat).sum(0).float().item()\n",
    "        return num_correct / labels.size(0)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loaders, optimizer, writer, num_epoch=10, device='cpu'):\n",
    "    # for logging to tensorboard\n",
    "    loss = {}\n",
    "    top1 = {}\n",
    "    top3 = {}\n",
    "    def run_epoch(mode):\n",
    "        epoch_loss = 0.0\n",
    "        epoch_top1 = 0.0\n",
    "        epoch_top3 = 0.0\n",
    "        for i, batch in enumerate(loaders[mode], 0):\n",
    "            in_data, labels = batch\n",
    "            in_data, labels = in_data.to(device), labels.to(device)\n",
    "\n",
    "            if mode == 'train':\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            logits = model(in_data)\n",
    "            batch_loss = model.loss(logits, labels)\n",
    "            batch_top1 = model.top_k_acc(logits, labels, k=1)\n",
    "            batch_top3 = model.top_k_acc(logits, labels, k=3)\n",
    "\n",
    "            epoch_loss += batch_loss.item()\n",
    "            epoch_top1 += batch_top1\n",
    "            epoch_top3 += batch_top3\n",
    "\n",
    "            if mode == 'train':\n",
    "                batch_loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        # sum of all batchs / num of batches\n",
    "        epoch_loss /= i + 1 \n",
    "        epoch_top1 /= i + 1\n",
    "        epoch_top3 /= i + 1\n",
    "        \n",
    "        loss[mode] = epoch_loss\n",
    "        top1[mode] = epoch_top1\n",
    "        top3[mode] = epoch_top3\n",
    "        \n",
    "        print('epoch %d %s loss %.4f top1 %.4f top3 %.4f' % (epoch, mode, epoch_loss, epoch_top1, epoch_top3))\n",
    "        # log to tensorboard\n",
    "        if not (writer is None) and (mode == 'eval'):\n",
    "            writer.add_scalars('%s_loss' % model.__class__.__name__,\n",
    "                         tag_scalar_dict={'train': loss['train'], \n",
    "                                          'eval': loss['eval']}, \n",
    "                         global_step=epoch)\n",
    "            writer.add_scalars('%s_top1' % model.__class__.__name__,\n",
    "                         tag_scalar_dict={'train': top1['train'], \n",
    "                                          'eval': top1['eval']}, \n",
    "                         global_step=epoch)\n",
    "            writer.add_scalars('%s_top3' % model.__class__.__name__,\n",
    "                         tag_scalar_dict={'train': top3['train'], \n",
    "                                          'eval': top3['eval']}, \n",
    "                         global_step=epoch)\n",
    "    for epoch in range(num_epoch):\n",
    "        run_epoch('train')\n",
    "        run_epoch('eval')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32_adam_0.001\n",
      "epoch 0 train loss 2.7049 top1 0.2475 top3 0.4809\n",
      "epoch 0 eval loss 1.9404 top1 0.4255 top3 0.7171\n",
      "epoch 1 train loss 1.6144 top1 0.5145 top3 0.7978\n",
      "epoch 1 eval loss 1.3761 top1 0.5809 top3 0.8514\n",
      "epoch 2 train loss 1.2455 top1 0.6189 top3 0.8689\n",
      "epoch 2 eval loss 1.1413 top1 0.6457 top3 0.8865\n",
      "epoch 3 train loss 1.0540 top1 0.6701 top3 0.9006\n",
      "epoch 3 eval loss 1.0126 top1 0.6790 top3 0.9074\n",
      "epoch 4 train loss 0.9481 top1 0.6980 top3 0.9158\n",
      "epoch 4 eval loss 0.9374 top1 0.6997 top3 0.9179\n",
      "epoch 5 train loss 0.8793 top1 0.7187 top3 0.9254\n",
      "epoch 5 eval loss 0.9004 top1 0.7157 top3 0.9217\n",
      "epoch 6 train loss 0.8359 top1 0.7310 top3 0.9316\n",
      "epoch 6 eval loss 0.8352 top1 0.7311 top3 0.9311\n",
      "epoch 7 train loss 0.7954 top1 0.7415 top3 0.9378\n",
      "epoch 7 eval loss 0.8287 top1 0.7300 top3 0.9329\n",
      "epoch 8 train loss 0.7651 top1 0.7505 top3 0.9420\n",
      "epoch 8 eval loss 0.7804 top1 0.7492 top3 0.9373\n",
      "epoch 9 train loss 0.7398 top1 0.7590 top3 0.9454\n",
      "epoch 9 eval loss 0.7729 top1 0.7459 top3 0.9394\n",
      "32_sgd_0.1\n",
      "epoch 0 train loss 3.8521 top1 0.0212 top3 0.0650\n",
      "epoch 0 eval loss 3.8517 top1 0.0192 top3 0.0578\n",
      "epoch 1 train loss 3.8509 top1 0.0215 top3 0.0651\n",
      "epoch 1 eval loss 3.8513 top1 0.0202 top3 0.0637\n",
      "epoch 2 train loss 3.8507 top1 0.0214 top3 0.0664\n",
      "epoch 2 eval loss 3.8520 top1 0.0192 top3 0.0620\n",
      "epoch 3 train loss 3.8507 top1 0.0218 top3 0.0652\n",
      "epoch 3 eval loss 3.8519 top1 0.0204 top3 0.0627\n",
      "epoch 4 train loss 3.8506 top1 0.0216 top3 0.0652\n",
      "epoch 4 eval loss 3.8518 top1 0.0192 top3 0.0658\n",
      "epoch 5 train loss 3.8508 top1 0.0219 top3 0.0644\n",
      "epoch 5 eval loss 3.8517 top1 0.0198 top3 0.0593\n",
      "epoch 6 train loss 3.8507 top1 0.0217 top3 0.0657\n",
      "epoch 6 eval loss 3.8516 top1 0.0231 top3 0.0658\n",
      "epoch 7 train loss 3.8506 top1 0.0215 top3 0.0654\n",
      "epoch 7 eval loss 3.8521 top1 0.0192 top3 0.0658\n",
      "epoch 8 train loss 3.8507 top1 0.0218 top3 0.0674\n",
      "epoch 8 eval loss 3.8513 top1 0.0208 top3 0.0615\n",
      "epoch 9 train loss 3.8507 top1 0.0213 top3 0.0641\n",
      "epoch 9 eval loss 3.8516 top1 0.0231 top3 0.0637\n",
      "32_sgd_0.01\n",
      "epoch 0 train loss 3.8541 top1 0.0218 top3 0.0640\n",
      "epoch 0 eval loss 3.8528 top1 0.0193 top3 0.0601\n",
      "epoch 1 train loss 3.8511 top1 0.0214 top3 0.0647\n",
      "epoch 1 eval loss 3.8517 top1 0.0202 top3 0.0631\n",
      "epoch 2 train loss 3.8503 top1 0.0220 top3 0.0665\n",
      "epoch 2 eval loss 3.8513 top1 0.0193 top3 0.0626\n",
      "epoch 3 train loss 3.8501 top1 0.0215 top3 0.0674\n",
      "epoch 3 eval loss 3.8513 top1 0.0193 top3 0.0610\n",
      "epoch 4 train loss 3.8500 top1 0.0221 top3 0.0662\n",
      "epoch 4 eval loss 3.8514 top1 0.0231 top3 0.0659\n",
      "epoch 5 train loss 3.8500 top1 0.0229 top3 0.0669\n",
      "epoch 5 eval loss 3.8512 top1 0.0231 top3 0.0669\n",
      "epoch 6 train loss 3.8500 top1 0.0227 top3 0.0665\n",
      "epoch 6 eval loss 3.8512 top1 0.0231 top3 0.0639\n",
      "epoch 7 train loss 3.8500 top1 0.0227 top3 0.0654\n",
      "epoch 7 eval loss 3.8513 top1 0.0231 top3 0.0664\n",
      "epoch 8 train loss 3.8500 top1 0.0230 top3 0.0664\n",
      "epoch 8 eval loss 3.8512 top1 0.0231 top3 0.0664\n",
      "epoch 9 train loss 3.8499 top1 0.0228 top3 0.0673\n",
      "epoch 9 eval loss 3.8513 top1 0.0231 top3 0.0658\n",
      "64_adam_0.001\n",
      "epoch 0 train loss 2.5312 top1 0.2956 top3 0.5321\n",
      "epoch 0 eval loss 1.6076 top1 0.5166 top3 0.8022\n",
      "epoch 1 train loss 1.3634 top1 0.5804 top3 0.8485\n",
      "epoch 1 eval loss 1.1927 top1 0.6259 top3 0.8787\n",
      "epoch 2 train loss 1.0841 top1 0.6575 top3 0.8983\n",
      "epoch 2 eval loss 1.0594 top1 0.6616 top3 0.9006\n",
      "epoch 3 train loss 0.9573 top1 0.6939 top3 0.9171\n",
      "epoch 3 eval loss 0.9833 top1 0.6794 top3 0.9120\n",
      "epoch 4 train loss 0.8747 top1 0.7200 top3 0.9289\n",
      "epoch 4 eval loss 0.8769 top1 0.7131 top3 0.9271\n",
      "epoch 5 train loss 0.8158 top1 0.7358 top3 0.9371\n",
      "epoch 5 eval loss 0.8401 top1 0.7300 top3 0.9301\n",
      "epoch 6 train loss 0.7740 top1 0.7478 top3 0.9431\n",
      "epoch 6 eval loss 0.7763 top1 0.7493 top3 0.9383\n",
      "epoch 7 train loss 0.7342 top1 0.7583 top3 0.9470\n",
      "epoch 7 eval loss 0.7771 top1 0.7455 top3 0.9399\n",
      "epoch 8 train loss 0.7065 top1 0.7675 top3 0.9507\n",
      "epoch 8 eval loss 0.7343 top1 0.7618 top3 0.9447\n",
      "epoch 9 train loss 0.6807 top1 0.7763 top3 0.9539\n",
      "epoch 9 eval loss 0.7142 top1 0.7644 top3 0.9474\n",
      "64_sgd_0.1\n",
      "epoch 0 train loss 3.8519 top1 0.0222 top3 0.0648\n",
      "epoch 0 eval loss 3.8515 top1 0.0203 top3 0.0641\n",
      "epoch 1 train loss 3.8508 top1 0.0215 top3 0.0651\n",
      "epoch 1 eval loss 3.8515 top1 0.0208 top3 0.0632\n",
      "epoch 2 train loss 3.8508 top1 0.0220 top3 0.0647\n",
      "epoch 2 eval loss 3.8515 top1 0.0198 top3 0.0621\n",
      "epoch 3 train loss 3.8508 top1 0.0214 top3 0.0641\n",
      "epoch 3 eval loss 3.8514 top1 0.0231 top3 0.0658\n",
      "epoch 4 train loss 3.8507 top1 0.0212 top3 0.0637\n",
      "epoch 4 eval loss 3.8515 top1 0.0231 top3 0.0636\n",
      "epoch 5 train loss 3.8507 top1 0.0216 top3 0.0652\n",
      "epoch 5 eval loss 3.8520 top1 0.0198 top3 0.0598\n",
      "epoch 6 train loss 3.8507 top1 0.0213 top3 0.0647\n",
      "epoch 6 eval loss 3.8521 top1 0.0198 top3 0.0576\n",
      "epoch 7 train loss 3.8506 top1 0.0214 top3 0.0647\n",
      "epoch 7 eval loss 3.8515 top1 0.0233 top3 0.0672\n",
      "epoch 8 train loss 3.8507 top1 0.0221 top3 0.0661\n",
      "epoch 8 eval loss 3.8508 top1 0.0206 top3 0.0672\n",
      "epoch 9 train loss 3.8507 top1 0.0221 top3 0.0658\n",
      "epoch 9 eval loss 3.8515 top1 0.0231 top3 0.0631\n",
      "64_sgd_0.01\n",
      "epoch 0 train loss 3.8531 top1 0.0210 top3 0.0622\n",
      "epoch 0 eval loss 3.8506 top1 0.0222 top3 0.0568\n",
      "epoch 1 train loss 3.8504 top1 0.0224 top3 0.0644\n",
      "epoch 1 eval loss 3.8510 top1 0.0198 top3 0.0613\n",
      "epoch 2 train loss 3.8501 top1 0.0218 top3 0.0664\n",
      "epoch 2 eval loss 3.8510 top1 0.0231 top3 0.0664\n",
      "epoch 3 train loss 3.8500 top1 0.0220 top3 0.0646\n",
      "epoch 3 eval loss 3.8512 top1 0.0231 top3 0.0621\n",
      "epoch 4 train loss 3.8500 top1 0.0227 top3 0.0657\n",
      "epoch 4 eval loss 3.8509 top1 0.0231 top3 0.0664\n",
      "epoch 5 train loss 3.8500 top1 0.0229 top3 0.0662\n",
      "epoch 5 eval loss 3.8510 top1 0.0231 top3 0.0664\n",
      "epoch 6 train loss 3.8499 top1 0.0230 top3 0.0658\n",
      "epoch 6 eval loss 3.8510 top1 0.0231 top3 0.0664\n",
      "epoch 7 train loss 3.8499 top1 0.0213 top3 0.0661\n",
      "epoch 7 eval loss 3.8509 top1 0.0231 top3 0.0663\n",
      "epoch 8 train loss 3.8498 top1 0.0229 top3 0.0658\n",
      "epoch 8 eval loss 3.8509 top1 0.0231 top3 0.0664\n",
      "epoch 9 train loss 3.8498 top1 0.0229 top3 0.0665\n",
      "epoch 9 eval loss 3.8510 top1 0.0231 top3 0.0664\n"
     ]
    }
   ],
   "source": [
    "for n_hidden in [32, 64]:\n",
    "    for optim_conf in [\n",
    "        {'optim':'adam', 'lr':0.001},\n",
    "        {'optim':'sgd', 'lr':0.1},\n",
    "        {'optim':'sgd', 'lr':0.01}\n",
    "    ]:\n",
    "        model = CnnClassifier(n_hidden=n_hidden)\n",
    "        if optim_conf['optim'] == 'adam':\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=optim_conf['lr'])\n",
    "        else:\n",
    "            optimizer = torch.optim.SGD(model.parameters(), lr=optim_conf['lr'])\n",
    "        conf_str = str(n_hidden)+'_'+optim_conf['optim']+'_'+str(optim_conf['lr'])\n",
    "        print(conf_str)\n",
    "        train(\n",
    "            model=model,\n",
    "            loaders={\n",
    "                'train': torch.utils.data.DataLoader(holdout_train_data, batch_size=32, shuffle=True),\n",
    "                'eval': torch.utils.data.DataLoader(holdout_eval_data, batch_size=32, shuffle=True)\n",
    "            },\n",
    "            optimizer=optimizer, \n",
    "            writer=SummaryWriter('./logs/cnn_scratch/%s' % (conf_str)), \n",
    "            num_epoch=10, \n",
    "            device='cpu'\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
