{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN from scratch\n",
    "\n",
    "## Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from tensorboardX import SummaryWriter\n",
    "from pa2_sample_code import get_datasets\n",
    "\n",
    "train_data, eval_data = get_datasets()\n",
    "\n",
    "# split train set into holdout_train and holdout_eval sets\n",
    "holdout_train_len = int(len(train_data) * 0.8)\n",
    "holdout_eval_len = len(train_data) - holdout_train_len\n",
    "holdout_train_data, holdout_eval_data = torch.utils.data.random_split(train_data, [holdout_train_len, holdout_eval_len])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declare model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CnnClassifier(nn.Module):\n",
    "    def __init__(self, n_hidden):\n",
    "        super(CnnClassifier, self).__init__()\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=4, kernel_size=3, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=4, out_channels=8, kernel_size=3, stride=2, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3, stride=2, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=1, padding=0),\n",
    "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=0),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.predictor = nn.Sequential(\n",
    "            nn.Linear(in_features=32, out_features=n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=n_hidden, out_features=47)\n",
    "        )\n",
    "        \n",
    "        self.loss_func = nn.CrossEntropyLoss(reduction='sum')\n",
    "        \n",
    "    def forward(self, in_data):\n",
    "        img_features = self.encoder(in_data).view(in_data.size(0), 32)\n",
    "        logits = self.predictor(img_features)\n",
    "        return logits\n",
    "\n",
    "    def loss(self, logits, labels):\n",
    "        return self.loss_func(logits, labels) / logits.size(0)\n",
    "    \n",
    "    def top_k_acc(self, logits, labels, k=1):\n",
    "        _, k_labels_pred = torch.topk(logits, k=k, dim=1) # shape (n, k)\n",
    "        k_labels = labels.unsqueeze(dim=1).expand(-1, k) # broadcast from (n) to (n, 1) to (n, k)\n",
    "        # flatten tensors for comparison\n",
    "        k_labels_pred_flat = k_labels_pred.reshape(1,-1).squeeze()\n",
    "        k_labels_flat = k_labels.reshape(1,-1).squeeze()\n",
    "        # get num_correct\n",
    "        num_correct = k_labels_pred_flat.eq(k_labels_flat).sum(0)\n",
    "        return num_correct / labels.size(0)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loaders, optimizer, writer, num_epoch=10, device='cpu'):\n",
    "    # for logging to tensorboard\n",
    "    loss = {}\n",
    "    top1 = {}\n",
    "    top3 = {}\n",
    "    def run_epoch(mode):\n",
    "        epoch_loss = 0.0\n",
    "        epoch_top1 = 0.0\n",
    "        epoch_top3 = 0.0\n",
    "        for i, batch in enumerate(loaders[mode], 0):\n",
    "            in_data, labels = batch\n",
    "            in_data, labels = in_data.to(device), labels.to(device)\n",
    "\n",
    "            if mode == 'train':\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            logits = model(in_data)\n",
    "            batch_loss = model.loss(logits, labels)\n",
    "            batch_top1 = model.top_k_acc(logits, labels, k=1)\n",
    "            batch_top3 = model.top_k_acc(logits, labels, k=3)\n",
    "\n",
    "            epoch_loss += batch_loss.item()\n",
    "            epoch_top1 += batch_top1\n",
    "            epoch_top3 += batch_top3\n",
    "\n",
    "            if mode == 'train':\n",
    "                batch_loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        # sum of all batchs / num of batches\n",
    "        epoch_loss /= i + 1 \n",
    "        epoch_top1 /= i + 1\n",
    "        epoch_top3 /= i + 1\n",
    "        \n",
    "        loss[mode] = epoch_loss\n",
    "        top1[mode] = epoch_top1\n",
    "        top3[mode] = epoch_top3\n",
    "        \n",
    "        print('epoch %d %s loss %.4f top1 %.4f top3 %.4f' % (epoch, mode, epoch_loss, epoch_top1, epoch_top3))\n",
    "        # log to tensorboard\n",
    "        if not (writer is None) and (mode == 'eval'):\n",
    "            writer.add_scalars('%s_loss' % model.__class__.__name__,\n",
    "                         tag_scalar_dict={'train': loss['train'], \n",
    "                                          'eval': loss['eval']}, \n",
    "                         global_step=epoch)\n",
    "            writer.add_scalars('%s_top1' % model.__class__.__name__,\n",
    "                         tag_scalar_dict={'train': top1['train'], \n",
    "                                          'eval': top1['eval']}, \n",
    "                         global_step=epoch)\n",
    "            writer.add_scalars('%s_top3' % model.__class__.__name__,\n",
    "                         tag_scalar_dict={'train': top3['train'], \n",
    "                                          'eval': top3['eval']}, \n",
    "                         global_step=epoch)\n",
    "    for epoch in range(num_epoch):\n",
    "        run_epoch('train')\n",
    "        run_epoch('eval')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32_adam_0.001\n",
      "epoch 0 train loss 2.5608 top1 0.0000 top3 0.0000\n",
      "epoch 0 eval loss 1.6268 top1 0.0000 top3 0.0000\n",
      "epoch 1 train loss 1.3624 top1 0.0000 top3 0.0000\n",
      "epoch 1 eval loss 1.1975 top1 0.0000 top3 0.0000\n",
      "epoch 2 train loss 1.1254 top1 0.0000 top3 0.0000\n",
      "epoch 2 eval loss 1.1101 top1 0.0000 top3 0.0000\n",
      "epoch 3 train loss 1.0144 top1 0.0000 top3 0.0000\n",
      "epoch 3 eval loss 0.9709 top1 0.0000 top3 0.0000\n",
      "epoch 4 train loss 0.9394 top1 0.0000 top3 0.0000\n",
      "epoch 4 eval loss 0.9671 top1 0.0000 top3 0.0000\n",
      "epoch 5 train loss 0.8839 top1 0.0000 top3 0.0000\n",
      "epoch 5 eval loss 0.8709 top1 0.0000 top3 0.0000\n",
      "epoch 6 train loss 0.8385 top1 0.0000 top3 0.0000\n",
      "epoch 6 eval loss 0.8439 top1 0.0000 top3 0.0000\n",
      "epoch 7 train loss 0.8024 top1 0.0000 top3 0.0000\n",
      "epoch 7 eval loss 0.8351 top1 0.0000 top3 0.0000\n",
      "epoch 8 train loss 0.7703 top1 0.0000 top3 0.0000\n",
      "epoch 8 eval loss 0.8045 top1 0.0000 top3 0.0000\n",
      "epoch 9 train loss 0.7438 top1 0.0000 top3 0.0000\n",
      "epoch 9 eval loss 0.7911 top1 0.0000 top3 0.0000\n",
      "32_sgd_0.1\n",
      "epoch 0 train loss 3.8520 top1 0.0000 top3 0.0000\n",
      "epoch 0 eval loss 3.8511 top1 0.0000 top3 0.0000\n",
      "epoch 1 train loss 3.8511 top1 0.0000 top3 0.0000\n",
      "epoch 1 eval loss 3.8509 top1 0.0000 top3 0.0000\n",
      "epoch 2 train loss 3.8509 top1 0.0000 top3 0.0000\n",
      "epoch 2 eval loss 3.8516 top1 0.0000 top3 0.0000\n",
      "epoch 3 train loss 3.8510 top1 0.0000 top3 0.0000\n",
      "epoch 3 eval loss 3.8511 top1 0.0000 top3 0.0000\n",
      "epoch 4 train loss 3.8508 top1 0.0000 top3 0.0000\n",
      "epoch 4 eval loss 3.8511 top1 0.0000 top3 0.0000\n",
      "epoch 5 train loss 3.8509 top1 0.0000 top3 0.0000\n",
      "epoch 5 eval loss 3.8506 top1 0.0000 top3 0.0000\n",
      "epoch 6 train loss 3.8508 top1 0.0000 top3 0.0000\n",
      "epoch 6 eval loss 3.8507 top1 0.0000 top3 0.0000\n",
      "epoch 7 train loss 3.8508 top1 0.0000 top3 0.0000\n",
      "epoch 7 eval loss 3.8507 top1 0.0000 top3 0.0000\n",
      "epoch 8 train loss 3.8509 top1 0.0000 top3 0.0000\n",
      "epoch 8 eval loss 3.8510 top1 0.0000 top3 0.0000\n",
      "epoch 9 train loss 3.8509 top1 0.0000 top3 0.0000\n",
      "epoch 9 eval loss 3.8507 top1 0.0000 top3 0.0000\n",
      "32_sgd_0.01\n",
      "epoch 0 train loss 3.8543 top1 0.0000 top3 0.0000\n",
      "epoch 0 eval loss 3.8517 top1 0.0000 top3 0.0000\n",
      "epoch 1 train loss 3.8513 top1 0.0000 top3 0.0000\n",
      "epoch 1 eval loss 3.8506 top1 0.0000 top3 0.0000\n",
      "epoch 2 train loss 3.8505 top1 0.0000 top3 0.0000\n",
      "epoch 2 eval loss 3.8505 top1 0.0000 top3 0.0000\n",
      "epoch 3 train loss 3.8502 top1 0.0000 top3 0.0000\n",
      "epoch 3 eval loss 3.8504 top1 0.0000 top3 0.0000\n",
      "epoch 4 train loss 3.8501 top1 0.0000 top3 0.0000\n",
      "epoch 4 eval loss 3.8504 top1 0.0000 top3 0.0000\n",
      "epoch 5 train loss 3.8500 top1 0.0000 top3 0.0000\n",
      "epoch 5 eval loss 3.8504 top1 0.0000 top3 0.0000\n"
     ]
    }
   ],
   "source": [
    "for n_hidden in [32, 64]:\n",
    "    for optimizer_conf in [\n",
    "        {'optimizer':'adam', 'lr':0.001},\n",
    "        {'optimizer':'sgd', 'lr':0.1},\n",
    "        {'optimizer':'sgd', 'lr':0.01},\n",
    "    ]:\n",
    "        model = CnnClassifier(n_hidden=n_hidden)\n",
    "        if optimizer_conf['optimizer'] == 'adam':\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=optimizer_conf['lr'])\n",
    "        else:\n",
    "            optimizer = torch.optim.SGD(model.parameters(), lr=optimizer_conf['lr'])\n",
    "        conf_str = str(n_hidden)+'_'+optimizer_conf['optimizer']+'_'+str(optimizer_conf['lr'])\n",
    "        print(conf_str)\n",
    "        train(\n",
    "            model=model,\n",
    "            loaders={\n",
    "                'train': torch.utils.data.DataLoader(holdout_train_data, batch_size=32, shuffle=True),\n",
    "                'eval': torch.utils.data.DataLoader(holdout_eval_data, batch_size=32)\n",
    "            },\n",
    "            optimizer=optimizer, \n",
    "            writer=SummaryWriter('./logs/cnn/%s' % (conf_str)), \n",
    "            num_epoch=10, \n",
    "            device='cpu'\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
