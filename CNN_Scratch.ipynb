{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN from scratch\n",
    "\n",
    "## Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from tensorboardX import SummaryWriter\n",
    "from pa2_sample_code import get_datasets\n",
    "\n",
    "train_data, eval_data = get_datasets()\n",
    "\n",
    "# split train set into holdout_train and holdout_eval sets\n",
    "holdout_train_len = int(len(train_data) * 0.8)\n",
    "holdout_eval_len = len(train_data) - holdout_train_len\n",
    "holdout_train_data, holdout_eval_data = torch.utils.data.random_split(train_data, [holdout_train_len, holdout_eval_len])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CnnFromScratch(nn.Module):\n",
    "    def __init__(self, n_hidden):\n",
    "        super(CnnFromScratch, self).__init__()\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=4, kernel_size=3, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=4, out_channels=8, kernel_size=3, stride=2, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3, stride=2, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=1, padding=0),\n",
    "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=0),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.predictor = nn.Sequential(\n",
    "            nn.Linear(in_features=32, out_features=n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=n_hidden, out_features=47)\n",
    "        )\n",
    "        \n",
    "        self.loss_func = nn.CrossEntropyLoss(reduction='sum')\n",
    "        \n",
    "    def forward(self, in_data):\n",
    "        img_features = self.encoder(in_data).view(in_data.size(0), 32)\n",
    "        logits = self.predictor(img_features)\n",
    "        return logits\n",
    "\n",
    "    def loss(self, logits, labels):\n",
    "        return self.loss_func(logits, labels) / logits.size(0)\n",
    "    \n",
    "    def top_k_acc(self, logits, labels, k=1):\n",
    "        _, k_labels_pred = torch.topk(logits, k=k, dim=1) # shape (n, k)\n",
    "        k_labels = labels.unsqueeze(dim=1).expand(-1, k) # broadcast from (n) to (n, 1) to (n, k)\n",
    "        # flatten tensors for comparison\n",
    "        k_labels_pred_flat = k_labels_pred.reshape(1,-1).squeeze()\n",
    "        k_labels_flat = k_labels.reshape(1,-1).squeeze()\n",
    "        # get num_correct in float\n",
    "        num_correct = k_labels_pred_flat.eq(k_labels_flat).sum(0).float().item()\n",
    "        return num_correct / labels.size(0)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refactored runner function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(model, loaders, optimizer, writer, num_epoch=10, device='cpu'):\n",
    "    def run_epoch(mode):\n",
    "        epoch_loss = 0.0\n",
    "        epoch_top1 = 0.0\n",
    "        epoch_top3 = 0.0\n",
    "        for i, batch in enumerate(loaders[mode], 0):\n",
    "            in_data, labels = batch\n",
    "            in_data, labels = in_data.to(device), labels.to(device)\n",
    "\n",
    "            if mode == 'train':\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            logits = model(in_data)\n",
    "            batch_loss = model.loss(logits, labels)\n",
    "            batch_top1 = model.top_k_acc(logits, labels, k=1)\n",
    "            batch_top3 = model.top_k_acc(logits, labels, k=3)\n",
    "\n",
    "            epoch_loss += batch_loss.item()\n",
    "            epoch_top1 += batch_top1\n",
    "            epoch_top3 += batch_top3\n",
    "\n",
    "            if mode == 'train':\n",
    "                batch_loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        # sum of all batchs / num of batches\n",
    "        epoch_loss /= i + 1 \n",
    "        epoch_top1 /= i + 1\n",
    "        epoch_top3 /= i + 1\n",
    "        \n",
    "        print('epoch %d %s loss %.4f top1 %.4f top3 %.4f' % (epoch, mode, epoch_loss, epoch_top1, epoch_top3))\n",
    "        # log to tensorboard\n",
    "        if not (writer is None):\n",
    "            writer.add_scalars('%s_loss' % model.__class__.__name__,\n",
    "                         tag_scalar_dict={mode: epoch_loss}, \n",
    "                         global_step=epoch)\n",
    "            writer.add_scalars('%s_top1' % model.__class__.__name__,\n",
    "                         tag_scalar_dict={mode: epoch_top1}, \n",
    "                         global_step=epoch)\n",
    "            writer.add_scalars('%s_top3' % model.__class__.__name__,\n",
    "                         tag_scalar_dict={mode: epoch_top3}, \n",
    "                         global_step=epoch)\n",
    "    for epoch in range(num_epoch):\n",
    "        run_epoch('train')\n",
    "        run_epoch('eval')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Holdout validation for choosing hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32_adam_0.001\n",
      "epoch 0 train loss 2.8782 top1 0.2135 top3 0.4182\n",
      "epoch 0 eval loss 2.0605 top1 0.3944 top3 0.6891\n",
      "epoch 1 train loss 1.7449 top1 0.4777 top3 0.7673\n",
      "epoch 1 eval loss 1.5698 top1 0.5270 top3 0.8091\n",
      "epoch 2 train loss 1.3929 top1 0.5760 top3 0.8448\n",
      "epoch 2 eval loss 1.3536 top1 0.5829 top3 0.8504\n",
      "epoch 3 train loss 1.1874 top1 0.6328 top3 0.8810\n",
      "epoch 3 eval loss 1.2666 top1 0.6076 top3 0.8664\n",
      "epoch 4 train loss 1.0622 top1 0.6669 top3 0.9001\n",
      "epoch 4 eval loss 1.0640 top1 0.6694 top3 0.8981\n",
      "epoch 5 train loss 0.9827 top1 0.6890 top3 0.9120\n",
      "epoch 5 eval loss 0.9844 top1 0.6887 top3 0.9129\n",
      "epoch 6 train loss 0.9265 top1 0.7039 top3 0.9193\n",
      "epoch 6 eval loss 0.9332 top1 0.7059 top3 0.9209\n",
      "epoch 7 train loss 0.8821 top1 0.7181 top3 0.9268\n",
      "epoch 7 eval loss 0.9212 top1 0.7084 top3 0.9198\n",
      "epoch 8 train loss 0.8477 top1 0.7283 top3 0.9313\n",
      "epoch 8 eval loss 0.8868 top1 0.7211 top3 0.9238\n",
      "epoch 9 train loss 0.8172 top1 0.7363 top3 0.9355\n",
      "epoch 9 eval loss 0.8360 top1 0.7324 top3 0.9325\n",
      "32_sgd_0.1\n",
      "epoch 0 train loss 3.8524 top1 0.0208 top3 0.0644\n",
      "epoch 0 eval loss 3.8515 top1 0.0205 top3 0.0670\n",
      "epoch 1 train loss 3.8511 top1 0.0220 top3 0.0655\n",
      "epoch 1 eval loss 3.8513 top1 0.0205 top3 0.0620\n",
      "epoch 2 train loss 3.8509 top1 0.0223 top3 0.0658\n",
      "epoch 2 eval loss 3.8517 top1 0.0205 top3 0.0644\n",
      "epoch 3 train loss 3.8507 top1 0.0228 top3 0.0675\n",
      "epoch 3 eval loss 3.8513 top1 0.0205 top3 0.0594\n",
      "epoch 4 train loss 3.8448 top1 0.0263 top3 0.0740\n",
      "epoch 4 eval loss 3.7890 top1 0.0334 top3 0.0946\n",
      "epoch 5 train loss 3.1364 top1 0.1284 top3 0.3075\n",
      "epoch 5 eval loss 2.3286 top1 0.3040 top3 0.6001\n",
      "epoch 6 train loss 1.7591 top1 0.4525 top3 0.7530\n",
      "epoch 6 eval loss 1.3860 top1 0.5621 top3 0.8494\n",
      "epoch 7 train loss 1.2019 top1 0.6098 top3 0.8803\n",
      "epoch 7 eval loss 1.1029 top1 0.6473 top3 0.8936\n",
      "epoch 8 train loss 1.0103 top1 0.6700 top3 0.9094\n",
      "epoch 8 eval loss 1.0019 top1 0.6788 top3 0.9123\n",
      "epoch 9 train loss 0.9079 top1 0.7035 top3 0.9254\n",
      "epoch 9 eval loss 0.8832 top1 0.7116 top3 0.9286\n",
      "32_sgd_0.01\n",
      "epoch 0 train loss 3.8558 top1 0.0212 top3 0.0628\n",
      "epoch 0 eval loss 3.8523 top1 0.0207 top3 0.0637\n",
      "epoch 1 train loss 3.8513 top1 0.0223 top3 0.0655\n",
      "epoch 1 eval loss 3.8510 top1 0.0210 top3 0.0637\n",
      "epoch 2 train loss 3.8505 top1 0.0213 top3 0.0651\n",
      "epoch 2 eval loss 3.8509 top1 0.0205 top3 0.0649\n",
      "epoch 3 train loss 3.8502 top1 0.0227 top3 0.0677\n",
      "epoch 3 eval loss 3.8507 top1 0.0205 top3 0.0622\n",
      "epoch 4 train loss 3.8500 top1 0.0235 top3 0.0667\n",
      "epoch 4 eval loss 3.8509 top1 0.0205 top3 0.0622\n",
      "epoch 5 train loss 3.8500 top1 0.0237 top3 0.0672\n",
      "epoch 5 eval loss 3.8509 top1 0.0205 top3 0.0617\n",
      "epoch 6 train loss 3.8500 top1 0.0237 top3 0.0666\n",
      "epoch 6 eval loss 3.8511 top1 0.0205 top3 0.0617\n",
      "epoch 7 train loss 3.8500 top1 0.0237 top3 0.0677\n",
      "epoch 7 eval loss 3.8511 top1 0.0205 top3 0.0640\n",
      "epoch 8 train loss 3.8500 top1 0.0237 top3 0.0674\n",
      "epoch 8 eval loss 3.8510 top1 0.0205 top3 0.0644\n",
      "epoch 9 train loss 3.8500 top1 0.0237 top3 0.0676\n",
      "epoch 9 eval loss 3.8509 top1 0.0205 top3 0.0623\n",
      "64_adam_0.001\n",
      "epoch 0 train loss 2.3714 top1 0.3408 top3 0.5806\n",
      "epoch 0 eval loss 1.4634 top1 0.5500 top3 0.8360\n",
      "epoch 1 train loss 1.2148 top1 0.6149 top3 0.8767\n",
      "epoch 1 eval loss 1.1086 top1 0.6468 top3 0.8959\n",
      "epoch 2 train loss 0.9899 top1 0.6827 top3 0.9111\n",
      "epoch 2 eval loss 0.9664 top1 0.6952 top3 0.9171\n",
      "epoch 3 train loss 0.8744 top1 0.7191 top3 0.9285\n",
      "epoch 3 eval loss 0.8582 top1 0.7267 top3 0.9311\n",
      "epoch 4 train loss 0.7979 top1 0.7392 top3 0.9391\n",
      "epoch 4 eval loss 0.8132 top1 0.7396 top3 0.9360\n",
      "epoch 5 train loss 0.7406 top1 0.7568 top3 0.9471\n",
      "epoch 5 eval loss 0.7755 top1 0.7448 top3 0.9405\n",
      "epoch 6 train loss 0.6989 top1 0.7711 top3 0.9512\n",
      "epoch 6 eval loss 0.7437 top1 0.7605 top3 0.9460\n",
      "epoch 7 train loss 0.6654 top1 0.7791 top3 0.9559\n",
      "epoch 7 eval loss 0.7153 top1 0.7714 top3 0.9460\n",
      "epoch 8 train loss 0.6352 top1 0.7897 top3 0.9584\n",
      "epoch 8 eval loss 0.7147 top1 0.7725 top3 0.9476\n",
      "epoch 9 train loss 0.6167 top1 0.7924 top3 0.9607\n",
      "epoch 9 eval loss 0.6833 top1 0.7776 top3 0.9531\n",
      "64_sgd_0.1\n",
      "epoch 0 train loss 3.8524 top1 0.0219 top3 0.0642\n",
      "epoch 0 eval loss 3.8510 top1 0.0205 top3 0.0648\n",
      "epoch 1 train loss 3.8509 top1 0.0224 top3 0.0648\n",
      "epoch 1 eval loss 3.8514 top1 0.0205 top3 0.0640\n",
      "epoch 2 train loss 3.8507 top1 0.0230 top3 0.0676\n",
      "epoch 2 eval loss 3.8510 top1 0.0205 top3 0.0801\n",
      "epoch 3 train loss 3.8390 top1 0.0246 top3 0.0729\n",
      "epoch 3 eval loss 3.7060 top1 0.0425 top3 0.1384\n",
      "epoch 4 train loss 2.8333 top1 0.1907 top3 0.4072\n",
      "epoch 4 eval loss 2.0511 top1 0.3821 top3 0.6848\n",
      "epoch 5 train loss 1.5276 top1 0.5284 top3 0.8091\n",
      "epoch 5 eval loss 1.1480 top1 0.6291 top3 0.8898\n",
      "epoch 6 train loss 1.0347 top1 0.6655 top3 0.9048\n",
      "epoch 6 eval loss 0.9041 top1 0.7104 top3 0.9243\n",
      "epoch 7 train loss 0.8709 top1 0.7152 top3 0.9285\n",
      "epoch 7 eval loss 0.8375 top1 0.7228 top3 0.9337\n",
      "epoch 8 train loss 0.7867 top1 0.7397 top3 0.9401\n",
      "epoch 8 eval loss 0.7893 top1 0.7472 top3 0.9397\n",
      "epoch 9 train loss 0.7267 top1 0.7579 top3 0.9474\n",
      "epoch 9 eval loss 0.7601 top1 0.7500 top3 0.9379\n",
      "64_sgd_0.01\n",
      "epoch 0 train loss 3.8546 top1 0.0199 top3 0.0607\n",
      "epoch 0 eval loss 3.8512 top1 0.0232 top3 0.0666\n",
      "epoch 1 train loss 3.8505 top1 0.0231 top3 0.0650\n",
      "epoch 1 eval loss 3.8508 top1 0.0205 top3 0.0627\n",
      "epoch 2 train loss 3.8503 top1 0.0236 top3 0.0662\n",
      "epoch 2 eval loss 3.8506 top1 0.0205 top3 0.0654\n",
      "epoch 3 train loss 3.8503 top1 0.0232 top3 0.0661\n",
      "epoch 3 eval loss 3.8510 top1 0.0205 top3 0.0618\n",
      "epoch 4 train loss 3.8502 top1 0.0237 top3 0.0672\n",
      "epoch 4 eval loss 3.8507 top1 0.0205 top3 0.0647\n",
      "epoch 5 train loss 3.8502 top1 0.0235 top3 0.0671\n",
      "epoch 5 eval loss 3.8509 top1 0.0205 top3 0.0644\n",
      "epoch 6 train loss 3.8502 top1 0.0237 top3 0.0666\n",
      "epoch 6 eval loss 3.8509 top1 0.0205 top3 0.0644\n",
      "epoch 7 train loss 3.8502 top1 0.0232 top3 0.0674\n",
      "epoch 7 eval loss 3.8508 top1 0.0205 top3 0.0616\n",
      "epoch 8 train loss 3.8501 top1 0.0237 top3 0.0672\n",
      "epoch 8 eval loss 3.8507 top1 0.0205 top3 0.0640\n",
      "epoch 9 train loss 3.8501 top1 0.0237 top3 0.0668\n",
      "epoch 9 eval loss 3.8508 top1 0.0205 top3 0.0640\n"
     ]
    }
   ],
   "source": [
    "for n_hidden in [32, 64]:\n",
    "    for optim_conf in [\n",
    "        {'optim':'adam', 'lr':0.001},\n",
    "        {'optim':'sgd', 'lr':0.1},\n",
    "        {'optim':'sgd', 'lr':0.01}\n",
    "    ]:\n",
    "        model = CnnFromScratch(n_hidden=n_hidden)\n",
    "        if optim_conf['optim'] == 'adam':\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=optim_conf['lr'])\n",
    "        else:\n",
    "            optimizer = torch.optim.SGD(model.parameters(), lr=optim_conf['lr'])\n",
    "        conf_str = str(n_hidden)+'_'+optim_conf['optim']+'_'+str(optim_conf['lr'])\n",
    "        print(conf_str)\n",
    "        run(\n",
    "            model=model,\n",
    "            loaders={\n",
    "                'train': torch.utils.data.DataLoader(holdout_train_data, batch_size=32, shuffle=True),\n",
    "                'eval': torch.utils.data.DataLoader(holdout_eval_data, batch_size=32, shuffle=True)\n",
    "            },\n",
    "            optimizer=optimizer, \n",
    "            writer=SummaryWriter('./logs/cnn_scratch/%s' % (conf_str)), \n",
    "            num_epoch=10, \n",
    "            device='cpu'\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training final model\n",
    "Hyper-parameter Selected:\n",
    "- Hidden layer size: 32\n",
    "- Optimization: Adam, learning rate 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_0\n",
      "epoch 0 train loss 2.3090 top1 0.3533 top3 0.5909\n",
      "epoch 0 eval loss 1.4254 top1 0.5661 top3 0.8350\n",
      "epoch 1 train loss 1.1889 top1 0.6300 top3 0.8812\n",
      "epoch 1 eval loss 1.0889 top1 0.6597 top3 0.8972\n",
      "epoch 2 train loss 0.9718 top1 0.6904 top3 0.9136\n",
      "epoch 2 eval loss 0.9607 top1 0.6961 top3 0.9144\n",
      "epoch 3 train loss 0.8717 top1 0.7208 top3 0.9273\n",
      "epoch 3 eval loss 0.8733 top1 0.7206 top3 0.9282\n",
      "epoch 4 train loss 0.8051 top1 0.7394 top3 0.9362\n",
      "epoch 4 eval loss 0.7971 top1 0.7448 top3 0.9377\n",
      "epoch 5 train loss 0.7500 top1 0.7570 top3 0.9437\n",
      "epoch 5 eval loss 0.7634 top1 0.7581 top3 0.9405\n",
      "epoch 6 train loss 0.7106 top1 0.7676 top3 0.9488\n",
      "epoch 6 eval loss 0.7768 top1 0.7474 top3 0.9392\n",
      "epoch 7 train loss 0.6789 top1 0.7786 top3 0.9528\n",
      "epoch 7 eval loss 0.6929 top1 0.7730 top3 0.9539\n",
      "epoch 8 train loss 0.6528 top1 0.7849 top3 0.9560\n",
      "epoch 8 eval loss 0.6798 top1 0.7782 top3 0.9512\n",
      "epoch 9 train loss 0.6323 top1 0.7897 top3 0.9572\n",
      "epoch 9 eval loss 0.6597 top1 0.7864 top3 0.9540\n",
      "epoch 10 train loss 0.6133 top1 0.7969 top3 0.9606\n",
      "epoch 10 eval loss 0.6500 top1 0.7868 top3 0.9566\n",
      "epoch 11 train loss 0.5993 top1 0.7988 top3 0.9621\n",
      "epoch 11 eval loss 0.6623 top1 0.7809 top3 0.9531\n",
      "epoch 12 train loss 0.5847 top1 0.8056 top3 0.9633\n",
      "epoch 12 eval loss 0.6050 top1 0.8002 top3 0.9606\n",
      "epoch 13 train loss 0.5726 top1 0.8088 top3 0.9648\n",
      "epoch 13 eval loss 0.6367 top1 0.7932 top3 0.9570\n",
      "epoch 14 train loss 0.5616 top1 0.8113 top3 0.9658\n",
      "epoch 14 eval loss 0.5996 top1 0.8001 top3 0.9609\n",
      "epoch 15 train loss 0.5515 top1 0.8146 top3 0.9673\n",
      "epoch 15 eval loss 0.6091 top1 0.7979 top3 0.9603\n",
      "epoch 16 train loss 0.5425 top1 0.8166 top3 0.9679\n",
      "epoch 16 eval loss 0.5898 top1 0.8061 top3 0.9631\n",
      "epoch 17 train loss 0.5327 top1 0.8201 top3 0.9695\n",
      "epoch 17 eval loss 0.6182 top1 0.7960 top3 0.9590\n",
      "epoch 18 train loss 0.5276 top1 0.8206 top3 0.9700\n",
      "epoch 18 eval loss 0.5849 top1 0.8069 top3 0.9629\n",
      "epoch 19 train loss 0.5213 top1 0.8237 top3 0.9701\n",
      "epoch 19 eval loss 0.5804 top1 0.8117 top3 0.9631\n",
      "final_1\n",
      "epoch 0 train loss 2.2736 top1 0.3631 top3 0.6090\n",
      "epoch 0 eval loss 1.4722 top1 0.5446 top3 0.8308\n",
      "epoch 1 train loss 1.2448 top1 0.6114 top3 0.8696\n",
      "epoch 1 eval loss 1.1318 top1 0.6457 top3 0.8887\n",
      "epoch 2 train loss 1.0481 top1 0.6702 top3 0.9032\n",
      "epoch 2 eval loss 1.0160 top1 0.6735 top3 0.9059\n",
      "epoch 3 train loss 0.9467 top1 0.6973 top3 0.9168\n",
      "epoch 3 eval loss 0.9662 top1 0.6956 top3 0.9136\n",
      "epoch 4 train loss 0.8832 top1 0.7166 top3 0.9265\n",
      "epoch 4 eval loss 0.9444 top1 0.7025 top3 0.9164\n",
      "epoch 5 train loss 0.8364 top1 0.7299 top3 0.9316\n",
      "epoch 5 eval loss 0.8544 top1 0.7315 top3 0.9296\n",
      "epoch 6 train loss 0.7992 top1 0.7437 top3 0.9368\n",
      "epoch 6 eval loss 0.8296 top1 0.7360 top3 0.9330\n",
      "epoch 7 train loss 0.7681 top1 0.7524 top3 0.9410\n",
      "epoch 7 eval loss 0.8202 top1 0.7382 top3 0.9342\n",
      "epoch 8 train loss 0.7427 top1 0.7584 top3 0.9446\n",
      "epoch 8 eval loss 0.7677 top1 0.7533 top3 0.9405\n",
      "epoch 9 train loss 0.7197 top1 0.7655 top3 0.9462\n",
      "epoch 9 eval loss 0.7668 top1 0.7475 top3 0.9439\n",
      "epoch 10 train loss 0.7019 top1 0.7690 top3 0.9489\n",
      "epoch 10 eval loss 0.7435 top1 0.7597 top3 0.9442\n",
      "epoch 11 train loss 0.6823 top1 0.7766 top3 0.9513\n",
      "epoch 11 eval loss 0.7378 top1 0.7628 top3 0.9440\n",
      "epoch 12 train loss 0.6665 top1 0.7809 top3 0.9536\n",
      "epoch 12 eval loss 0.7262 top1 0.7649 top3 0.9468\n",
      "epoch 13 train loss 0.6537 top1 0.7838 top3 0.9553\n",
      "epoch 13 eval loss 0.6965 top1 0.7723 top3 0.9513\n",
      "epoch 14 train loss 0.6396 top1 0.7881 top3 0.9566\n",
      "epoch 14 eval loss 0.7094 top1 0.7654 top3 0.9470\n",
      "epoch 15 train loss 0.6297 top1 0.7894 top3 0.9575\n",
      "epoch 15 eval loss 0.6900 top1 0.7787 top3 0.9503\n",
      "epoch 16 train loss 0.6217 top1 0.7919 top3 0.9585\n",
      "epoch 16 eval loss 0.6645 top1 0.7826 top3 0.9546\n",
      "epoch 17 train loss 0.6091 top1 0.7966 top3 0.9605\n",
      "epoch 17 eval loss 0.6936 top1 0.7706 top3 0.9524\n",
      "epoch 18 train loss 0.5982 top1 0.8004 top3 0.9616\n",
      "epoch 18 eval loss 0.6925 top1 0.7722 top3 0.9519\n",
      "epoch 19 train loss 0.5906 top1 0.8008 top3 0.9625\n",
      "epoch 19 eval loss 0.6746 top1 0.7795 top3 0.9537\n",
      "final_2\n",
      "epoch 0 train loss 2.5816 top1 0.2725 top3 0.5073\n",
      "epoch 0 eval loss 1.6595 top1 0.4976 top3 0.7884\n",
      "epoch 1 train loss 1.3644 top1 0.5750 top3 0.8494\n",
      "epoch 1 eval loss 1.2234 top1 0.6079 top3 0.8727\n",
      "epoch 2 train loss 1.0797 top1 0.6562 top3 0.8980\n",
      "epoch 2 eval loss 1.0353 top1 0.6694 top3 0.9044\n",
      "epoch 3 train loss 0.9473 top1 0.6941 top3 0.9187\n",
      "epoch 3 eval loss 0.9101 top1 0.7044 top3 0.9235\n",
      "epoch 4 train loss 0.8637 top1 0.7190 top3 0.9301\n",
      "epoch 4 eval loss 0.8505 top1 0.7243 top3 0.9327\n",
      "epoch 5 train loss 0.8040 top1 0.7376 top3 0.9388\n",
      "epoch 5 eval loss 0.7927 top1 0.7451 top3 0.9396\n",
      "epoch 6 train loss 0.7609 top1 0.7512 top3 0.9429\n",
      "epoch 6 eval loss 0.7897 top1 0.7423 top3 0.9380\n",
      "epoch 7 train loss 0.7263 top1 0.7614 top3 0.9478\n",
      "epoch 7 eval loss 0.7477 top1 0.7607 top3 0.9446\n",
      "epoch 8 train loss 0.7009 top1 0.7695 top3 0.9494\n",
      "epoch 8 eval loss 0.7389 top1 0.7580 top3 0.9465\n",
      "epoch 9 train loss 0.6779 top1 0.7767 top3 0.9532\n",
      "epoch 9 eval loss 0.7110 top1 0.7669 top3 0.9492\n",
      "epoch 10 train loss 0.6595 top1 0.7823 top3 0.9555\n",
      "epoch 10 eval loss 0.6856 top1 0.7754 top3 0.9529\n",
      "epoch 11 train loss 0.6409 top1 0.7894 top3 0.9570\n",
      "epoch 11 eval loss 0.6684 top1 0.7759 top3 0.9537\n",
      "epoch 12 train loss 0.6279 top1 0.7914 top3 0.9595\n",
      "epoch 12 eval loss 0.6785 top1 0.7798 top3 0.9526\n",
      "epoch 13 train loss 0.6120 top1 0.7969 top3 0.9610\n",
      "epoch 13 eval loss 0.6460 top1 0.7840 top3 0.9583\n",
      "epoch 14 train loss 0.6014 top1 0.7997 top3 0.9618\n",
      "epoch 14 eval loss 0.6563 top1 0.7825 top3 0.9557\n",
      "epoch 15 train loss 0.5925 top1 0.8017 top3 0.9630\n",
      "epoch 15 eval loss 0.6417 top1 0.7829 top3 0.9589\n",
      "epoch 16 train loss 0.5830 top1 0.8047 top3 0.9638\n",
      "epoch 16 eval loss 0.6405 top1 0.7865 top3 0.9576\n",
      "epoch 17 train loss 0.5754 top1 0.8065 top3 0.9650\n",
      "epoch 17 eval loss 0.6229 top1 0.7941 top3 0.9596\n",
      "epoch 18 train loss 0.5659 top1 0.8107 top3 0.9656\n",
      "epoch 18 eval loss 0.6150 top1 0.7972 top3 0.9593\n",
      "epoch 19 train loss 0.5596 top1 0.8116 top3 0.9670\n",
      "epoch 19 eval loss 0.6333 top1 0.7896 top3 0.9562\n",
      "final_3\n",
      "epoch 0 train loss 2.2612 top1 0.3631 top3 0.6081\n",
      "epoch 0 eval loss 1.3874 top1 0.5750 top3 0.8474\n",
      "epoch 1 train loss 1.1210 top1 0.6441 top3 0.8932\n",
      "epoch 1 eval loss 0.9750 top1 0.6847 top3 0.9157\n",
      "epoch 2 train loss 0.9046 top1 0.7077 top3 0.9256\n",
      "epoch 2 eval loss 0.8722 top1 0.7188 top3 0.9297\n",
      "epoch 3 train loss 0.8022 top1 0.7384 top3 0.9393\n",
      "epoch 3 eval loss 0.8038 top1 0.7389 top3 0.9406\n",
      "epoch 4 train loss 0.7372 top1 0.7557 top3 0.9478\n",
      "epoch 4 eval loss 0.7363 top1 0.7584 top3 0.9474\n",
      "epoch 5 train loss 0.6910 top1 0.7733 top3 0.9524\n",
      "epoch 5 eval loss 0.6901 top1 0.7718 top3 0.9513\n",
      "epoch 6 train loss 0.6593 top1 0.7805 top3 0.9566\n",
      "epoch 6 eval loss 0.6756 top1 0.7748 top3 0.9543\n",
      "epoch 7 train loss 0.6287 top1 0.7900 top3 0.9605\n",
      "epoch 7 eval loss 0.6692 top1 0.7799 top3 0.9546\n",
      "epoch 8 train loss 0.6052 top1 0.7959 top3 0.9621\n",
      "epoch 8 eval loss 0.6418 top1 0.7866 top3 0.9571\n",
      "epoch 9 train loss 0.5883 top1 0.8027 top3 0.9648\n",
      "epoch 9 eval loss 0.6232 top1 0.7929 top3 0.9597\n",
      "epoch 10 train loss 0.5702 top1 0.8076 top3 0.9672\n",
      "epoch 10 eval loss 0.6081 top1 0.7945 top3 0.9604\n",
      "epoch 11 train loss 0.5577 top1 0.8117 top3 0.9685\n",
      "epoch 11 eval loss 0.6104 top1 0.7973 top3 0.9612\n",
      "epoch 12 train loss 0.5475 top1 0.8150 top3 0.9688\n",
      "epoch 12 eval loss 0.6020 top1 0.7984 top3 0.9603\n",
      "epoch 13 train loss 0.5337 top1 0.8190 top3 0.9700\n",
      "epoch 13 eval loss 0.5929 top1 0.7993 top3 0.9628\n",
      "epoch 14 train loss 0.5274 top1 0.8195 top3 0.9705\n",
      "epoch 14 eval loss 0.5805 top1 0.8054 top3 0.9631\n",
      "epoch 15 train loss 0.5174 top1 0.8230 top3 0.9711\n",
      "epoch 15 eval loss 0.5969 top1 0.7988 top3 0.9632\n",
      "epoch 16 train loss 0.5106 top1 0.8247 top3 0.9725\n",
      "epoch 16 eval loss 0.5648 top1 0.8105 top3 0.9651\n",
      "epoch 17 train loss 0.5032 top1 0.8280 top3 0.9730\n",
      "epoch 17 eval loss 0.5624 top1 0.8104 top3 0.9645\n",
      "epoch 18 train loss 0.4971 top1 0.8280 top3 0.9740\n",
      "epoch 18 eval loss 0.5489 top1 0.8159 top3 0.9660\n",
      "epoch 19 train loss 0.4904 top1 0.8323 top3 0.9742\n",
      "epoch 19 eval loss 0.5522 top1 0.8149 top3 0.9670\n",
      "final_4\n",
      "epoch 0 train loss 2.2741 top1 0.3684 top3 0.5980\n",
      "epoch 0 eval loss 1.2852 top1 0.6091 top3 0.8631\n",
      "epoch 1 train loss 1.1219 top1 0.6486 top3 0.8910\n",
      "epoch 1 eval loss 1.0304 top1 0.6757 top3 0.9033\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 train loss 0.9588 top1 0.6952 top3 0.9166\n",
      "epoch 2 eval loss 0.9060 top1 0.7110 top3 0.9229\n",
      "epoch 3 train loss 0.8690 top1 0.7211 top3 0.9289\n",
      "epoch 3 eval loss 0.8761 top1 0.7222 top3 0.9285\n",
      "epoch 4 train loss 0.8084 top1 0.7390 top3 0.9375\n",
      "epoch 4 eval loss 0.8181 top1 0.7407 top3 0.9345\n",
      "epoch 5 train loss 0.7622 top1 0.7541 top3 0.9424\n",
      "epoch 5 eval loss 0.7863 top1 0.7499 top3 0.9406\n",
      "epoch 6 train loss 0.7227 top1 0.7646 top3 0.9471\n",
      "epoch 6 eval loss 0.7270 top1 0.7640 top3 0.9468\n",
      "epoch 7 train loss 0.6940 top1 0.7725 top3 0.9519\n",
      "epoch 7 eval loss 0.7259 top1 0.7651 top3 0.9454\n",
      "epoch 8 train loss 0.6656 top1 0.7816 top3 0.9540\n",
      "epoch 8 eval loss 0.6868 top1 0.7757 top3 0.9504\n",
      "epoch 9 train loss 0.6454 top1 0.7865 top3 0.9561\n",
      "epoch 9 eval loss 0.6703 top1 0.7788 top3 0.9546\n",
      "epoch 10 train loss 0.6289 top1 0.7907 top3 0.9593\n",
      "epoch 10 eval loss 0.6665 top1 0.7819 top3 0.9533\n",
      "epoch 11 train loss 0.6077 top1 0.7985 top3 0.9627\n",
      "epoch 11 eval loss 0.6397 top1 0.7895 top3 0.9572\n",
      "epoch 12 train loss 0.5975 top1 0.8004 top3 0.9630\n",
      "epoch 12 eval loss 0.6433 top1 0.7869 top3 0.9573\n",
      "epoch 13 train loss 0.5850 top1 0.8047 top3 0.9640\n",
      "epoch 13 eval loss 0.6512 top1 0.7869 top3 0.9553\n",
      "epoch 14 train loss 0.5739 top1 0.8079 top3 0.9657\n",
      "epoch 14 eval loss 0.6208 top1 0.7898 top3 0.9610\n",
      "epoch 15 train loss 0.5625 top1 0.8096 top3 0.9668\n",
      "epoch 15 eval loss 0.6144 top1 0.7934 top3 0.9612\n",
      "epoch 16 train loss 0.5544 top1 0.8142 top3 0.9677\n",
      "epoch 16 eval loss 0.6178 top1 0.7929 top3 0.9616\n",
      "epoch 17 train loss 0.5430 top1 0.8163 top3 0.9691\n",
      "epoch 17 eval loss 0.6066 top1 0.7996 top3 0.9627\n",
      "epoch 18 train loss 0.5347 top1 0.8181 top3 0.9705\n",
      "epoch 18 eval loss 0.5921 top1 0.7995 top3 0.9634\n",
      "epoch 19 train loss 0.5277 top1 0.8224 top3 0.9712\n",
      "epoch 19 eval loss 0.5796 top1 0.8039 top3 0.9651\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    model = CnnFromScratch(n_hidden=64)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    conf_str = 'final_'+str(i)\n",
    "    print(conf_str)\n",
    "    run(\n",
    "        model=model,\n",
    "        loaders={\n",
    "            'train': torch.utils.data.DataLoader(train_data, batch_size=32, shuffle=True),\n",
    "            'eval': torch.utils.data.DataLoader(eval_data, batch_size=32, shuffle=True)\n",
    "        },\n",
    "        optimizer=optimizer, \n",
    "        writer=SummaryWriter('./logs/cnn_scratch/%s' % (conf_str)), \n",
    "        num_epoch=20, \n",
    "        device='cpu'\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
