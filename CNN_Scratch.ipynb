{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN from scratch\n",
    "\n",
    "## Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from tensorboardX import SummaryWriter\n",
    "from pa2_sample_code import get_datasets\n",
    "\n",
    "train_data, eval_data = get_datasets()\n",
    "\n",
    "# split train set into holdout_train and holdout_eval sets\n",
    "holdout_train_len = int(len(train_data) * 0.8)\n",
    "holdout_eval_len = len(train_data) - holdout_train_len\n",
    "holdout_train_data, holdout_eval_data = torch.utils.data.random_split(train_data, [holdout_train_len, holdout_eval_len])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CnnFromScratch(nn.Module):\n",
    "    def __init__(self, n_hidden):\n",
    "        super(CnnFromScratch, self).__init__()\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=4, kernel_size=3, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=4, out_channels=8, kernel_size=3, stride=2, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3, stride=2, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=1, padding=0),\n",
    "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=0),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.predictor = nn.Sequential(\n",
    "            nn.Linear(in_features=32, out_features=n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=n_hidden, out_features=47)\n",
    "        )\n",
    "        \n",
    "        self.loss_func = nn.CrossEntropyLoss(reduction='sum')\n",
    "        \n",
    "    def forward(self, in_data):\n",
    "        img_features = self.encoder(in_data).view(in_data.size(0), 32)\n",
    "        logits = self.predictor(img_features)\n",
    "        return logits\n",
    "\n",
    "    def loss(self, logits, labels):\n",
    "        return self.loss_func(logits, labels) / logits.size(0)\n",
    "    \n",
    "    def top_k_acc(self, logits, labels, k=1):\n",
    "        _, k_labels_pred = torch.topk(logits, k=k, dim=1) # shape (n, k)\n",
    "        k_labels = labels.unsqueeze(dim=1).expand(-1, k) # broadcast from (n) to (n, 1) to (n, k)\n",
    "        # flatten tensors for comparison\n",
    "        k_labels_pred_flat = k_labels_pred.reshape(1,-1).squeeze()\n",
    "        k_labels_flat = k_labels.reshape(1,-1).squeeze()\n",
    "        # get num_correct in float\n",
    "        num_correct = k_labels_pred_flat.eq(k_labels_flat).sum(0).float().item()\n",
    "        return num_correct / labels.size(0)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refactored runner function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(model, loaders, optimizer, writer, num_epoch=10, device='cpu'):\n",
    "    def run_epoch(mode):\n",
    "        epoch_loss = 0.0\n",
    "        epoch_top1 = 0.0\n",
    "        epoch_top3 = 0.0\n",
    "        for i, batch in enumerate(loaders[mode], 0):\n",
    "            in_data, labels = batch\n",
    "            in_data, labels = in_data.to(device), labels.to(device)\n",
    "\n",
    "            if mode == 'train':\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            logits = model(in_data)\n",
    "            batch_loss = model.loss(logits, labels)\n",
    "            batch_top1 = model.top_k_acc(logits, labels, k=1)\n",
    "            batch_top3 = model.top_k_acc(logits, labels, k=3)\n",
    "\n",
    "            epoch_loss += batch_loss.item()\n",
    "            epoch_top1 += batch_top1\n",
    "            epoch_top3 += batch_top3\n",
    "\n",
    "            if mode == 'train':\n",
    "                batch_loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        # sum of all batchs / num of batches\n",
    "        epoch_loss /= i + 1 \n",
    "        epoch_top1 /= i + 1\n",
    "        epoch_top3 /= i + 1\n",
    "        \n",
    "        print('epoch %d %s loss %.4f top1 %.4f top3 %.4f' % (epoch, mode, epoch_loss, epoch_top1, epoch_top3))\n",
    "        # log to tensorboard\n",
    "        if not (writer is None):\n",
    "            writer.add_scalars('%s_loss' % model.__class__.__name__,\n",
    "                         tag_scalar_dict={mode: epoch_loss}, \n",
    "                         global_step=epoch)\n",
    "            writer.add_scalars('%s_top1' % model.__class__.__name__,\n",
    "                         tag_scalar_dict={mode: epoch_top1}, \n",
    "                         global_step=epoch)\n",
    "            writer.add_scalars('%s_top3' % model.__class__.__name__,\n",
    "                         tag_scalar_dict={mode: epoch_top3}, \n",
    "                         global_step=epoch)\n",
    "    for epoch in range(num_epoch):\n",
    "        run_epoch('train')\n",
    "        run_epoch('eval')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Holdout validation for choosing hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32_adam_0.001\n",
      "epoch 0 train loss 2.7725 top1 0.2398 top3 0.4626\n",
      "epoch 0 eval loss 1.7474 top1 0.4736 top3 0.7831\n",
      "epoch 1 train loss 1.4188 top1 0.5678 top3 0.8426\n",
      "epoch 1 eval loss 1.2034 top1 0.6316 top3 0.8846\n",
      "epoch 2 train loss 1.0835 top1 0.6607 top3 0.9008\n",
      "epoch 2 eval loss 1.0265 top1 0.6805 top3 0.9085\n",
      "epoch 3 train loss 0.9338 top1 0.7011 top3 0.9213\n",
      "epoch 3 eval loss 0.9174 top1 0.7073 top3 0.9253\n",
      "epoch 4 train loss 0.8451 top1 0.7272 top3 0.9344\n",
      "epoch 4 eval loss 0.8906 top1 0.7158 top3 0.9312\n",
      "epoch 5 train loss 0.7868 top1 0.7458 top3 0.9407\n",
      "epoch 5 eval loss 0.7939 top1 0.7429 top3 0.9403\n",
      "epoch 6 train loss 0.7403 top1 0.7581 top3 0.9461\n",
      "epoch 6 eval loss 0.7539 top1 0.7598 top3 0.9457\n",
      "epoch 7 train loss 0.7049 top1 0.7684 top3 0.9510\n",
      "epoch 7 eval loss 0.7017 top1 0.7751 top3 0.9498\n",
      "epoch 8 train loss 0.6731 top1 0.7794 top3 0.9541\n",
      "epoch 8 eval loss 0.6949 top1 0.7780 top3 0.9529\n",
      "epoch 9 train loss 0.6528 top1 0.7856 top3 0.9568\n",
      "epoch 9 eval loss 0.6839 top1 0.7775 top3 0.9522\n",
      "32_sgd_0.1\n",
      "epoch 0 train loss 3.8520 top1 0.0213 top3 0.0613\n",
      "epoch 0 eval loss 3.8508 top1 0.0227 top3 0.0643\n",
      "epoch 1 train loss 3.8510 top1 0.0227 top3 0.0664\n",
      "epoch 1 eval loss 3.8508 top1 0.0227 top3 0.0658\n",
      "epoch 2 train loss 3.8507 top1 0.0226 top3 0.0658\n",
      "epoch 2 eval loss 3.8504 top1 0.0209 top3 0.0634\n",
      "epoch 3 train loss 3.8509 top1 0.0217 top3 0.0652\n",
      "epoch 3 eval loss 3.8506 top1 0.0227 top3 0.0672\n",
      "epoch 4 train loss 3.8509 top1 0.0222 top3 0.0657\n",
      "epoch 4 eval loss 3.8504 top1 0.0227 top3 0.0664\n",
      "epoch 5 train loss 3.8508 top1 0.0211 top3 0.0660\n",
      "epoch 5 eval loss 3.8505 top1 0.0227 top3 0.0666\n",
      "epoch 6 train loss 3.8509 top1 0.0222 top3 0.0652\n",
      "epoch 6 eval loss 3.8505 top1 0.0228 top3 0.0677\n",
      "epoch 7 train loss 3.8509 top1 0.0217 top3 0.0656\n",
      "epoch 7 eval loss 3.8504 top1 0.0222 top3 0.0636\n",
      "epoch 8 train loss 3.8508 top1 0.0220 top3 0.0638\n",
      "epoch 8 eval loss 3.8508 top1 0.0227 top3 0.0656\n",
      "epoch 9 train loss 3.8509 top1 0.0215 top3 0.0652\n",
      "epoch 9 eval loss 3.8506 top1 0.0227 top3 0.0672\n",
      "32_sgd_0.01\n",
      "epoch 0 train loss 3.8547 top1 0.0220 top3 0.0646\n",
      "epoch 0 eval loss 3.8525 top1 0.0216 top3 0.0634\n",
      "epoch 1 train loss 3.8510 top1 0.0218 top3 0.0655\n",
      "epoch 1 eval loss 3.8510 top1 0.0216 top3 0.0643\n",
      "epoch 2 train loss 3.8502 top1 0.0221 top3 0.0659\n",
      "epoch 2 eval loss 3.8504 top1 0.0232 top3 0.0726\n",
      "epoch 3 train loss 3.8499 top1 0.0236 top3 0.0688\n",
      "epoch 3 eval loss 3.8501 top1 0.0227 top3 0.0741\n",
      "epoch 4 train loss 3.8497 top1 0.0231 top3 0.0693\n",
      "epoch 4 eval loss 3.8499 top1 0.0227 top3 0.0710\n",
      "epoch 5 train loss 3.8494 top1 0.0229 top3 0.0733\n",
      "epoch 5 eval loss 3.8496 top1 0.0227 top3 0.0739\n",
      "epoch 6 train loss 3.8490 top1 0.0236 top3 0.0741\n",
      "epoch 6 eval loss 3.8489 top1 0.0222 top3 0.0753\n",
      "epoch 7 train loss 3.8482 top1 0.0247 top3 0.0770\n",
      "epoch 7 eval loss 3.8479 top1 0.0262 top3 0.0856\n",
      "epoch 8 train loss 3.8466 top1 0.0292 top3 0.0870\n",
      "epoch 8 eval loss 3.8456 top1 0.0304 top3 0.0910\n",
      "epoch 9 train loss 3.8427 top1 0.0359 top3 0.0940\n",
      "epoch 9 eval loss 3.8397 top1 0.0327 top3 0.0922\n",
      "64_adam_0.001\n",
      "epoch 0 train loss 2.6462 top1 0.2615 top3 0.4941\n",
      "epoch 0 eval loss 1.7901 top1 0.4724 top3 0.7630\n",
      "epoch 1 train loss 1.4287 top1 0.5616 top3 0.8394\n",
      "epoch 1 eval loss 1.2289 top1 0.6170 top3 0.8778\n",
      "epoch 2 train loss 1.1215 top1 0.6446 top3 0.8918\n",
      "epoch 2 eval loss 1.0714 top1 0.6535 top3 0.8986\n",
      "epoch 3 train loss 0.9718 top1 0.6856 top3 0.9151\n",
      "epoch 3 eval loss 0.9317 top1 0.6987 top3 0.9224\n",
      "epoch 4 train loss 0.8767 top1 0.7151 top3 0.9295\n",
      "epoch 4 eval loss 0.8832 top1 0.7169 top3 0.9236\n",
      "epoch 5 train loss 0.8145 top1 0.7335 top3 0.9366\n",
      "epoch 5 eval loss 0.8018 top1 0.7389 top3 0.9371\n",
      "epoch 6 train loss 0.7645 top1 0.7499 top3 0.9434\n",
      "epoch 6 eval loss 0.7693 top1 0.7496 top3 0.9401\n",
      "epoch 7 train loss 0.7261 top1 0.7609 top3 0.9489\n",
      "epoch 7 eval loss 0.7390 top1 0.7600 top3 0.9441\n",
      "epoch 8 train loss 0.6973 top1 0.7704 top3 0.9524\n",
      "epoch 8 eval loss 0.7369 top1 0.7580 top3 0.9449\n",
      "epoch 9 train loss 0.6708 top1 0.7767 top3 0.9559\n",
      "epoch 9 eval loss 0.7061 top1 0.7669 top3 0.9500\n",
      "64_sgd_0.1\n",
      "epoch 0 train loss 3.8529 top1 0.0219 top3 0.0647\n",
      "epoch 0 eval loss 3.8509 top1 0.0227 top3 0.0648\n",
      "epoch 1 train loss 3.8512 top1 0.0211 top3 0.0642\n",
      "epoch 1 eval loss 3.8508 top1 0.0227 top3 0.0640\n",
      "epoch 2 train loss 3.8509 top1 0.0230 top3 0.0653\n",
      "epoch 2 eval loss 3.8507 top1 0.0227 top3 0.0655\n",
      "epoch 3 train loss 3.8509 top1 0.0223 top3 0.0648\n",
      "epoch 3 eval loss 3.8502 top1 0.0227 top3 0.0688\n",
      "epoch 4 train loss 3.8509 top1 0.0217 top3 0.0655\n",
      "epoch 4 eval loss 3.8509 top1 0.0227 top3 0.0653\n",
      "epoch 5 train loss 3.8509 top1 0.0214 top3 0.0628\n",
      "epoch 5 eval loss 3.8504 top1 0.0227 top3 0.0672\n",
      "epoch 6 train loss 3.8509 top1 0.0224 top3 0.0644\n",
      "epoch 6 eval loss 3.8502 top1 0.0228 top3 0.0658\n",
      "epoch 7 train loss 3.8508 top1 0.0227 top3 0.0659\n",
      "epoch 7 eval loss 3.8509 top1 0.0227 top3 0.0676\n",
      "epoch 8 train loss 3.8508 top1 0.0218 top3 0.0663\n",
      "epoch 8 eval loss 3.8506 top1 0.0201 top3 0.0638\n",
      "epoch 9 train loss 3.8509 top1 0.0218 top3 0.0649\n",
      "epoch 9 eval loss 3.8503 top1 0.0227 top3 0.0656\n",
      "64_sgd_0.01\n",
      "epoch 0 train loss 3.8521 top1 0.0213 top3 0.0639\n",
      "epoch 0 eval loss 3.8506 top1 0.0228 top3 0.0652\n",
      "epoch 1 train loss 3.8505 top1 0.0225 top3 0.0664\n",
      "epoch 1 eval loss 3.8504 top1 0.0227 top3 0.0663\n",
      "epoch 2 train loss 3.8503 top1 0.0230 top3 0.0671\n",
      "epoch 2 eval loss 3.8504 top1 0.0227 top3 0.0658\n",
      "epoch 3 train loss 3.8502 top1 0.0228 top3 0.0672\n",
      "epoch 3 eval loss 3.8502 top1 0.0227 top3 0.0663\n",
      "epoch 4 train loss 3.8502 top1 0.0228 top3 0.0668\n",
      "epoch 4 eval loss 3.8504 top1 0.0227 top3 0.0671\n",
      "epoch 5 train loss 3.8502 top1 0.0229 top3 0.0668\n",
      "epoch 5 eval loss 3.8504 top1 0.0227 top3 0.0672\n",
      "epoch 6 train loss 3.8502 top1 0.0231 top3 0.0667\n",
      "epoch 6 eval loss 3.8504 top1 0.0227 top3 0.0663\n",
      "epoch 7 train loss 3.8502 top1 0.0230 top3 0.0671\n",
      "epoch 7 eval loss 3.8504 top1 0.0227 top3 0.0658\n",
      "epoch 8 train loss 3.8502 top1 0.0228 top3 0.0664\n",
      "epoch 8 eval loss 3.8504 top1 0.0227 top3 0.0663\n",
      "epoch 9 train loss 3.8502 top1 0.0230 top3 0.0674\n",
      "epoch 9 eval loss 3.8503 top1 0.0227 top3 0.0663\n"
     ]
    }
   ],
   "source": [
    "for n_hidden in [32, 64]:\n",
    "    for optim_conf in [\n",
    "        {'optim':'adam', 'lr':0.001},\n",
    "        {'optim':'sgd', 'lr':0.1},\n",
    "        {'optim':'sgd', 'lr':0.01}\n",
    "    ]:\n",
    "        model = CnnFromScratch(n_hidden=n_hidden)\n",
    "        if optim_conf['optim'] == 'adam':\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=optim_conf['lr'])\n",
    "        else:\n",
    "            optimizer = torch.optim.SGD(model.parameters(), lr=optim_conf['lr'])\n",
    "        conf_str = str(n_hidden)+'_'+optim_conf['optim']+'_'+str(optim_conf['lr'])\n",
    "        print(conf_str)\n",
    "        run(\n",
    "            model=model,\n",
    "            loaders={\n",
    "                'train': torch.utils.data.DataLoader(holdout_train_data, batch_size=32, shuffle=True),\n",
    "                'eval': torch.utils.data.DataLoader(holdout_eval_data, batch_size=32, shuffle=True)\n",
    "            },\n",
    "            optimizer=optimizer, \n",
    "            writer=SummaryWriter('./logs/cnn_scratch/%s' % (conf_str)), \n",
    "            num_epoch=10, \n",
    "            device='cpu'\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training final model\n",
    "Hyper-parameter Selected:\n",
    "- Hidden layers: 64\n",
    "- Optimization: Adam, learning rate 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_0\n",
      "epoch 0 train loss 2.4859 top1 0.3039 top3 0.5359\n",
      "epoch 0 eval loss 1.5378 top1 0.5255 top3 0.8149\n",
      "epoch 1 train loss 1.2666 top1 0.6044 top3 0.8673\n",
      "epoch 1 eval loss 1.1240 top1 0.6439 top3 0.8873\n",
      "epoch 2 train loss 1.0364 top1 0.6720 top3 0.9032\n",
      "epoch 2 eval loss 1.0010 top1 0.6811 top3 0.9067\n",
      "epoch 3 train loss 0.9239 top1 0.7036 top3 0.9196\n",
      "epoch 3 eval loss 0.9013 top1 0.7052 top3 0.9226\n",
      "epoch 4 train loss 0.8480 top1 0.7281 top3 0.9297\n",
      "epoch 4 eval loss 0.8534 top1 0.7227 top3 0.9271\n",
      "epoch 5 train loss 0.7903 top1 0.7443 top3 0.9393\n",
      "epoch 5 eval loss 0.8013 top1 0.7426 top3 0.9368\n",
      "epoch 6 train loss 0.7527 top1 0.7546 top3 0.9435\n",
      "epoch 6 eval loss 0.7738 top1 0.7398 top3 0.9409\n",
      "epoch 7 train loss 0.7176 top1 0.7656 top3 0.9484\n",
      "epoch 7 eval loss 0.7760 top1 0.7447 top3 0.9392\n",
      "epoch 8 train loss 0.6937 top1 0.7722 top3 0.9511\n",
      "epoch 8 eval loss 0.7055 top1 0.7669 top3 0.9505\n",
      "epoch 9 train loss 0.6697 top1 0.7788 top3 0.9535\n",
      "epoch 9 eval loss 0.6968 top1 0.7698 top3 0.9496\n",
      "epoch 10 train loss 0.6488 top1 0.7849 top3 0.9565\n",
      "epoch 10 eval loss 0.6765 top1 0.7770 top3 0.9521\n",
      "epoch 11 train loss 0.6316 top1 0.7868 top3 0.9587\n",
      "epoch 11 eval loss 0.6766 top1 0.7769 top3 0.9522\n",
      "epoch 12 train loss 0.6155 top1 0.7949 top3 0.9603\n",
      "epoch 12 eval loss 0.6751 top1 0.7803 top3 0.9505\n",
      "epoch 13 train loss 0.6027 top1 0.7986 top3 0.9619\n",
      "epoch 13 eval loss 0.6476 top1 0.7846 top3 0.9543\n",
      "epoch 14 train loss 0.5917 top1 0.7998 top3 0.9629\n",
      "epoch 14 eval loss 0.6613 top1 0.7805 top3 0.9548\n",
      "epoch 15 train loss 0.5827 top1 0.8031 top3 0.9642\n",
      "epoch 15 eval loss 0.6490 top1 0.7835 top3 0.9566\n",
      "epoch 16 train loss 0.5711 top1 0.8078 top3 0.9658\n",
      "epoch 16 eval loss 0.6396 top1 0.7816 top3 0.9593\n",
      "epoch 17 train loss 0.5628 top1 0.8106 top3 0.9662\n",
      "epoch 17 eval loss 0.6393 top1 0.7891 top3 0.9582\n",
      "epoch 18 train loss 0.5538 top1 0.8136 top3 0.9677\n",
      "epoch 18 eval loss 0.6174 top1 0.7953 top3 0.9596\n",
      "epoch 19 train loss 0.5460 top1 0.8160 top3 0.9689\n",
      "epoch 19 eval loss 0.6014 top1 0.7966 top3 0.9607\n",
      "final_1\n",
      "epoch 0 train loss 2.3377 top1 0.3404 top3 0.5857\n",
      "epoch 0 eval loss 1.4377 top1 0.5657 top3 0.8338\n",
      "epoch 1 train loss 1.2304 top1 0.6138 top3 0.8735\n",
      "epoch 1 eval loss 1.0909 top1 0.6610 top3 0.8969\n",
      "epoch 2 train loss 1.0167 top1 0.6734 top3 0.9091\n",
      "epoch 2 eval loss 0.9867 top1 0.6926 top3 0.9132\n",
      "epoch 3 train loss 0.9040 top1 0.7080 top3 0.9255\n",
      "epoch 3 eval loss 0.8838 top1 0.7175 top3 0.9266\n",
      "epoch 4 train loss 0.8319 top1 0.7291 top3 0.9355\n",
      "epoch 4 eval loss 0.8404 top1 0.7302 top3 0.9317\n",
      "epoch 5 train loss 0.7784 top1 0.7469 top3 0.9415\n",
      "epoch 5 eval loss 0.8092 top1 0.7368 top3 0.9368\n",
      "epoch 6 train loss 0.7403 top1 0.7572 top3 0.9480\n",
      "epoch 6 eval loss 0.7536 top1 0.7531 top3 0.9436\n",
      "epoch 7 train loss 0.7081 top1 0.7686 top3 0.9505\n",
      "epoch 7 eval loss 0.7196 top1 0.7683 top3 0.9487\n",
      "epoch 8 train loss 0.6808 top1 0.7764 top3 0.9536\n",
      "epoch 8 eval loss 0.7264 top1 0.7635 top3 0.9468\n",
      "epoch 9 train loss 0.6612 top1 0.7805 top3 0.9556\n",
      "epoch 9 eval loss 0.7047 top1 0.7675 top3 0.9493\n",
      "epoch 10 train loss 0.6414 top1 0.7878 top3 0.9578\n",
      "epoch 10 eval loss 0.6725 top1 0.7785 top3 0.9533\n",
      "epoch 11 train loss 0.6266 top1 0.7914 top3 0.9595\n",
      "epoch 11 eval loss 0.6713 top1 0.7798 top3 0.9549\n",
      "epoch 12 train loss 0.6133 top1 0.7953 top3 0.9609\n",
      "epoch 12 eval loss 0.6691 top1 0.7800 top3 0.9543\n",
      "epoch 13 train loss 0.6021 top1 0.7987 top3 0.9630\n",
      "epoch 13 eval loss 0.6356 top1 0.7917 top3 0.9571\n",
      "epoch 14 train loss 0.5915 top1 0.8020 top3 0.9636\n",
      "epoch 14 eval loss 0.6278 top1 0.7952 top3 0.9596\n",
      "epoch 15 train loss 0.5822 top1 0.8026 top3 0.9649\n",
      "epoch 15 eval loss 0.6305 top1 0.7931 top3 0.9590\n",
      "epoch 16 train loss 0.5709 top1 0.8062 top3 0.9663\n",
      "epoch 16 eval loss 0.6443 top1 0.7837 top3 0.9553\n",
      "epoch 17 train loss 0.5614 top1 0.8101 top3 0.9675\n",
      "epoch 17 eval loss 0.6157 top1 0.7992 top3 0.9600\n",
      "epoch 18 train loss 0.5528 top1 0.8125 top3 0.9679\n",
      "epoch 18 eval loss 0.6140 top1 0.7968 top3 0.9615\n",
      "epoch 19 train loss 0.5477 top1 0.8127 top3 0.9681\n",
      "epoch 19 eval loss 0.6135 top1 0.7980 top3 0.9599\n",
      "final_2\n",
      "epoch 0 train loss 2.2722 top1 0.3559 top3 0.6065\n",
      "epoch 0 eval loss 1.5109 top1 0.5285 top3 0.8229\n",
      "epoch 1 train loss 1.2783 top1 0.5986 top3 0.8637\n",
      "epoch 1 eval loss 1.1513 top1 0.6407 top3 0.8860\n",
      "epoch 2 train loss 1.0562 top1 0.6626 top3 0.9008\n",
      "epoch 2 eval loss 1.0342 top1 0.6713 top3 0.9068\n",
      "epoch 3 train loss 0.9445 top1 0.6957 top3 0.9181\n",
      "epoch 3 eval loss 0.9197 top1 0.7068 top3 0.9209\n",
      "epoch 4 train loss 0.8705 top1 0.7197 top3 0.9274\n",
      "epoch 4 eval loss 0.8504 top1 0.7275 top3 0.9327\n",
      "epoch 5 train loss 0.8151 top1 0.7362 top3 0.9365\n",
      "epoch 5 eval loss 0.7954 top1 0.7432 top3 0.9370\n",
      "epoch 6 train loss 0.7713 top1 0.7487 top3 0.9411\n",
      "epoch 6 eval loss 0.7899 top1 0.7499 top3 0.9383\n",
      "epoch 7 train loss 0.7376 top1 0.7583 top3 0.9457\n",
      "epoch 7 eval loss 0.7623 top1 0.7517 top3 0.9401\n",
      "epoch 8 train loss 0.7070 top1 0.7687 top3 0.9497\n",
      "epoch 8 eval loss 0.7502 top1 0.7608 top3 0.9419\n",
      "epoch 9 train loss 0.6821 top1 0.7748 top3 0.9525\n",
      "epoch 9 eval loss 0.7011 top1 0.7703 top3 0.9492\n",
      "epoch 10 train loss 0.6584 top1 0.7822 top3 0.9565\n",
      "epoch 10 eval loss 0.7065 top1 0.7670 top3 0.9505\n",
      "epoch 11 train loss 0.6414 top1 0.7863 top3 0.9569\n",
      "epoch 11 eval loss 0.6705 top1 0.7780 top3 0.9531\n",
      "epoch 12 train loss 0.6291 top1 0.7908 top3 0.9589\n",
      "epoch 12 eval loss 0.6803 top1 0.7765 top3 0.9520\n",
      "epoch 13 train loss 0.6134 top1 0.7969 top3 0.9600\n",
      "epoch 13 eval loss 0.6649 top1 0.7838 top3 0.9537\n",
      "epoch 14 train loss 0.6038 top1 0.7975 top3 0.9623\n",
      "epoch 14 eval loss 0.6371 top1 0.7907 top3 0.9571\n",
      "epoch 15 train loss 0.5889 top1 0.8037 top3 0.9630\n",
      "epoch 15 eval loss 0.6327 top1 0.7929 top3 0.9574\n",
      "epoch 16 train loss 0.5829 top1 0.8024 top3 0.9646\n",
      "epoch 16 eval loss 0.6210 top1 0.7949 top3 0.9593\n",
      "epoch 17 train loss 0.5723 top1 0.8058 top3 0.9652\n",
      "epoch 17 eval loss 0.6406 top1 0.7910 top3 0.9565\n",
      "epoch 18 train loss 0.5635 top1 0.8077 top3 0.9659\n",
      "epoch 18 eval loss 0.6223 top1 0.7925 top3 0.9588\n",
      "epoch 19 train loss 0.5559 top1 0.8119 top3 0.9666\n",
      "epoch 19 eval loss 0.6186 top1 0.7944 top3 0.9600\n",
      "final_3\n",
      "epoch 0 train loss 2.5791 top1 0.2760 top3 0.5134\n",
      "epoch 0 eval loss 1.6991 top1 0.4882 top3 0.7790\n",
      "epoch 1 train loss 1.3786 top1 0.5720 top3 0.8444\n",
      "epoch 1 eval loss 1.1983 top1 0.6242 top3 0.8795\n",
      "epoch 2 train loss 1.0675 top1 0.6581 top3 0.9012\n",
      "epoch 2 eval loss 1.0109 top1 0.6830 top3 0.9078\n",
      "epoch 3 train loss 0.9330 top1 0.7005 top3 0.9211\n",
      "epoch 3 eval loss 0.9014 top1 0.7085 top3 0.9239\n",
      "epoch 4 train loss 0.8485 top1 0.7255 top3 0.9333\n",
      "epoch 4 eval loss 0.8624 top1 0.7200 top3 0.9300\n",
      "epoch 5 train loss 0.7921 top1 0.7418 top3 0.9389\n",
      "epoch 5 eval loss 0.8142 top1 0.7355 top3 0.9371\n",
      "epoch 6 train loss 0.7507 top1 0.7558 top3 0.9447\n",
      "epoch 6 eval loss 0.7517 top1 0.7568 top3 0.9444\n",
      "epoch 7 train loss 0.7190 top1 0.7647 top3 0.9496\n",
      "epoch 7 eval loss 0.7319 top1 0.7587 top3 0.9473\n",
      "epoch 8 train loss 0.6913 top1 0.7712 top3 0.9520\n",
      "epoch 8 eval loss 0.7192 top1 0.7636 top3 0.9486\n",
      "epoch 9 train loss 0.6692 top1 0.7798 top3 0.9545\n",
      "epoch 9 eval loss 0.7157 top1 0.7676 top3 0.9471\n",
      "epoch 10 train loss 0.6491 top1 0.7876 top3 0.9570\n",
      "epoch 10 eval loss 0.6594 top1 0.7812 top3 0.9552\n",
      "epoch 11 train loss 0.6335 top1 0.7888 top3 0.9595\n",
      "epoch 11 eval loss 0.6917 top1 0.7751 top3 0.9508\n",
      "epoch 12 train loss 0.6195 top1 0.7943 top3 0.9610\n",
      "epoch 12 eval loss 0.6512 top1 0.7851 top3 0.9566\n",
      "epoch 13 train loss 0.6056 top1 0.7982 top3 0.9621\n",
      "epoch 13 eval loss 0.6315 top1 0.7891 top3 0.9584\n",
      "epoch 14 train loss 0.5929 top1 0.8044 top3 0.9628\n",
      "epoch 14 eval loss 0.6282 top1 0.7861 top3 0.9600\n",
      "epoch 15 train loss 0.5827 top1 0.8061 top3 0.9650\n",
      "epoch 15 eval loss 0.6358 top1 0.7897 top3 0.9573\n",
      "epoch 16 train loss 0.5721 top1 0.8089 top3 0.9654\n",
      "epoch 16 eval loss 0.6181 top1 0.7933 top3 0.9598\n",
      "epoch 17 train loss 0.5670 top1 0.8109 top3 0.9662\n",
      "epoch 17 eval loss 0.6125 top1 0.7941 top3 0.9616\n",
      "epoch 18 train loss 0.5572 top1 0.8136 top3 0.9672\n",
      "epoch 18 eval loss 0.6109 top1 0.7960 top3 0.9624\n",
      "epoch 19 train loss 0.5469 top1 0.8171 top3 0.9686\n",
      "epoch 19 eval loss 0.5919 top1 0.8008 top3 0.9643\n",
      "final_4\n",
      "epoch 0 train loss 2.3116 top1 0.3484 top3 0.5915\n",
      "epoch 0 eval loss 1.4805 top1 0.5445 top3 0.8231\n",
      "epoch 1 train loss 1.2745 top1 0.5958 top3 0.8643\n",
      "epoch 1 eval loss 1.2006 top1 0.6186 top3 0.8764\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 train loss 1.0662 top1 0.6590 top3 0.9003\n",
      "epoch 2 eval loss 1.0111 top1 0.6770 top3 0.9062\n",
      "epoch 3 train loss 0.9445 top1 0.6979 top3 0.9179\n",
      "epoch 3 eval loss 0.9109 top1 0.7088 top3 0.9217\n",
      "epoch 4 train loss 0.8555 top1 0.7246 top3 0.9312\n",
      "epoch 4 eval loss 0.8829 top1 0.7159 top3 0.9230\n",
      "epoch 5 train loss 0.7980 top1 0.7421 top3 0.9387\n",
      "epoch 5 eval loss 0.8174 top1 0.7363 top3 0.9351\n",
      "epoch 6 train loss 0.7550 top1 0.7542 top3 0.9436\n",
      "epoch 6 eval loss 0.7587 top1 0.7560 top3 0.9413\n",
      "epoch 7 train loss 0.7235 top1 0.7645 top3 0.9469\n",
      "epoch 7 eval loss 0.7397 top1 0.7623 top3 0.9441\n",
      "epoch 8 train loss 0.6951 top1 0.7705 top3 0.9502\n",
      "epoch 8 eval loss 0.7274 top1 0.7652 top3 0.9463\n",
      "epoch 9 train loss 0.6730 top1 0.7791 top3 0.9537\n",
      "epoch 9 eval loss 0.7145 top1 0.7641 top3 0.9451\n",
      "epoch 10 train loss 0.6511 top1 0.7863 top3 0.9557\n",
      "epoch 10 eval loss 0.6797 top1 0.7812 top3 0.9509\n",
      "epoch 11 train loss 0.6357 top1 0.7906 top3 0.9583\n",
      "epoch 11 eval loss 0.6651 top1 0.7825 top3 0.9514\n",
      "epoch 12 train loss 0.6204 top1 0.7948 top3 0.9589\n",
      "epoch 12 eval loss 0.6719 top1 0.7822 top3 0.9516\n",
      "epoch 13 train loss 0.6081 top1 0.7984 top3 0.9608\n",
      "epoch 13 eval loss 0.6517 top1 0.7900 top3 0.9534\n",
      "epoch 14 train loss 0.5942 top1 0.8021 top3 0.9627\n",
      "epoch 14 eval loss 0.6464 top1 0.7893 top3 0.9547\n",
      "epoch 15 train loss 0.5848 top1 0.8041 top3 0.9638\n",
      "epoch 15 eval loss 0.6333 top1 0.7937 top3 0.9568\n",
      "epoch 16 train loss 0.5772 top1 0.8071 top3 0.9648\n",
      "epoch 16 eval loss 0.6204 top1 0.7956 top3 0.9569\n",
      "epoch 17 train loss 0.5661 top1 0.8082 top3 0.9669\n",
      "epoch 17 eval loss 0.6239 top1 0.7954 top3 0.9571\n",
      "epoch 18 train loss 0.5593 top1 0.8108 top3 0.9665\n",
      "epoch 18 eval loss 0.6419 top1 0.7879 top3 0.9582\n",
      "epoch 19 train loss 0.5508 top1 0.8141 top3 0.9684\n",
      "epoch 19 eval loss 0.6477 top1 0.7890 top3 0.9543\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    model = CnnFromScratch(n_hidden=64)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    conf_str = 'final_'+str(i)\n",
    "    print(conf_str)\n",
    "    run(\n",
    "        model=model,\n",
    "        loaders={\n",
    "            'train': torch.utils.data.DataLoader(train_data, batch_size=32, shuffle=True),\n",
    "            'eval': torch.utils.data.DataLoader(eval_data, batch_size=32, shuffle=True)\n",
    "        },\n",
    "        optimizer=optimizer, \n",
    "        writer=SummaryWriter('./logs/cnn_scratch/%s' % (conf_str)), \n",
    "        num_epoch=20, \n",
    "        device='cpu'\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
